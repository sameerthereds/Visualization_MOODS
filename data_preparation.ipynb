{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************\n",
      "data creating file started\n",
      "****************************\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "print(\"****************************\")\n",
    "print(\"data creating file started\")\n",
    "print(\"****************************\")\n",
    "from cerebralcortex import Kernel\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructField, StructType, LongType, \\\n",
    "DoubleType,MapType, StringType,ArrayType, FloatType, TimestampType, IntegerType,DateType\n",
    "from pyspark.sql.functions import col\n",
    "from cerebralcortex.util.helper_methods import get_study_names\n",
    "from datetime import datetime,timedelta,date\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "import schedule\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import calendar\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import copy\n",
    "import statistics\n",
    "from itertools import repeat\n",
    "import pickle\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please make sure the today date is of Friday. Lets say some error occured and you are supposed to run this again on Saturday, change the date to the Friday\n",
    "\n",
    "\n",
    "\n",
    "# today=datetime.datetime(2023,9,15).date()\n",
    "\n",
    "\n",
    "today=date.today()\n",
    "\n",
    "\n",
    "flag=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-04\n"
     ]
    }
   ],
   "source": [
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "include_withdrawn_completed_users=flag\n",
    "save_chi=flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/jupyter/sneupane/MOODS/pickled_files/active_complete_users.pickle', 'rb') as handle:\n",
    "#     active_complete_users = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users=['e7d06756-41c3-313c-93e2-1d0fb1cb83f1', 'd388d927-0131-48f8-aa16-403214ff2fe4', 'ef9c09c2-c77b-3734-808c-b611bd5e390b',\n",
    "            'fa16df27-3215-3cb1-b20d-949fe4b34736', '4dc8dafb-1e12-3100-8b0c-5940bd42c3fe', '12bb64cc-c191-3277-b7e3-f3cb64e20d9d', \n",
    "            'af5f95f0-09cc-3a0a-b283-1a0ae724e43c','910e36cd-2465-33e2-8f76-532606b4bf93', 'b9588ec7-2376-3f9a-bcab-bd91fbf84a23', \n",
    "            'e5c71b91-87c4-3011-8d9e-1fea87b09562', '136112f5-b332-34f7-89e9-4a0e6c5f54bd', 'afcfc1b5-365f-409b-918e-2f0ce8056ff9', \n",
    "            'e792585a-48a2-36ae-9a50-de0605f76829', 'a8974961-a56c-31c0-b63d-ac8224451881', 'e55b847f-4557-3bba-9cfd-84f261a9c8fc', \n",
    "            '14a6bacd-02f6-315a-9e5a-da99925f38e2',\"ae77f7c7-a007-3e9c-bfbf-2ee04e08665d\",  'cfe7d4d1-b4fa-3eb5-8f15-e78257e61cf3',\n",
    "            \"6b01f588-7b0d-3f45-afba-e3bb73b3a5a9\",\"be6ef5f1-e505-3f19-a8de-c7b653e764e1\",\"30573732-49c7-3be7-80ca-e4b24b68d6ca\",\n",
    "           \"22cb371a-70f3-3010-b21c-d0f0a235cf74\",\"d20e1bc7-de8d-38e4-9d97-09e64b88816d\",\"cb23a061-61f4-3600-b33e-7ae5e92890ad\",\n",
    "           \"fcb9a3a8-e7a6-3806-a7b8-edc1d4be4a60\",\"a99d7a50-23ff-3e19-95b7-e86e2f1dbd9c\",\"f47cbc29-dede-388e-895c-c30fc9196a3c\",\n",
    "           \"ac66b6c2-9aa9-30d6-98f7-9cb56e4dbeed\",\"9dd833f6-61a2-32f4-b7f6-e3760a3881d0\",\"85d371b9-8a3d-30f4-9daf-01d865dcc44d\",\n",
    "           \"bd52a846-bb06-31ad-9db2-b71e9b411931\",\"bd52a846-bb06-31ad-9db2-b71e9b411931\",\"3d8191a0-e885-376c-a0ff-2b3b5114aa03\",\"ec9f72a3-fd99-3871-9434-bce64155d8bc\",\"43741ae0-d461-359f-83df-89794559f0ce\",\n",
    "           \"f45190dd-c0ac-3275-aedc-69c3f9770202\",\"1fea1944-c1ea-3b39-a632-54188cf83d16\",\"8a29a1cd-b21d-3afa-98cb-2d374040c144\",\"61702538-9f21-3491-a2ea-dee71c24f5ff\",\n",
    "           \"c4ef872f-a0db-3a4b-beae-75b7ec7f4c6b\",\"2b0398a3-a92d-34f3-b5bc-e1293916a714\"]\n",
    "# \"ec9f72a3-fd99-3871-9434-bce64155d8bc\",\n",
    "# \"c4ef872f-a0db-3a4b-beae-75b7ec7f4c6b\" :\"Jody\"\n",
    "pilot_users=['7d9937db-7133-3f39-8c50-a70fc1a53f5c','35f7fd27-e07e-3332-8908-3638b66f01f8', 'b3991d3b-f8d0-3c64-9fc3-ad9a0b45d277',\n",
    "             '42159f10-f41f-3585-b040-540ab2dc4b93', 'c4d00fcf-0020-3a92-a9ee-5ecf38d137ed', '50ef4edc-3cc4-3955-a21b-07b51c7a6dd0',\n",
    " '7a6310bd-de3e-3a1b-a3bc-4671e3e6cf63', '1234aa13-c97c-3c5b-abcf-249ce49a45b5', '1e77b486-c4a0-3486-ade8-4b81037bb6dd', \n",
    "             '9550fabf-4992-369d-8ec8-df59a8818d59',  '8f9dbda4-8e32-3cb6-8c38-4d63058b1af9',\n",
    " 'a63dd8fb-704c-3b50-b62a-b40ed3f18476', 'b781f782-107d-3c3b-b8a3-9082e47c4212', '0cdf8895-f351-34de-b43d-890a1e7ead8f',\n",
    " 'e2018413-ca27-3537-8bf6-b45265515f27', 'd6f4844c-9688-33a3-9dca-e43debcbb839','bba3ac20-0490-36c2-b1b0-dc1585768620',\n",
    "             '547b6c56-2bf4-3393-9715-4536c08b6581','feba1d6a-063d-3540-8d9e-d63434a632df','a4ce4274-c9b4-36b6-96f2-61fbd3a41dc4',\n",
    "             '5fa0f0db-1e9c-3a1c-b78f-b26435482103','09de38a0-5d5f-3081-a8ca-2ab5fe5d7c72',\"84e96c23-1072-31cd-ab34-015b6c9c8b74\"]\n",
    "\n",
    "# remove '84e96c23-1072-31cd-ab34-015b6c9c8b74' from pilot users coz she is actively collecting data\n",
    "#kept connie startmail user in withdrawn_users too . she will join using gmail account\n",
    "# added duplicate users to withdraw too\n",
    "withdrawn_users=['bba3ac20-0490-36c2-b1b0-dc1585768620','547b6c56-2bf4-3393-9715-4536c08b6581','feba1d6a-063d-3540-8d9e-d63434a632df',\n",
    "                 'a4ce4274-c9b4-36b6-96f2-61fbd3a41dc4','5fa0f0db-1e9c-3a1c-b78f-b26435482103',\"0cdf8895-f351-34de-b43d-890a1e7ead8f\",\n",
    "                \"ed4fc8c8-e0b8-391e-9ea9-bbc09f94d9ab\",\"4779baba-3f7b-335d-8d94-a0b77a96c770\",\"016b995e-5433-3e33-887d-27b8be76e4e3\",\n",
    "                 \"cba9e411-4232-3db1-956a-f3890f0a56ee\",\"49b78324-abbd-3139-9bb1-a4397afba316\",\"cb4f9aba-52b0-3fda-97b0-9249a13dc84f\",\n",
    "                \"061c05b7-2650-364d-9b6f-a81ee7ed518f\",\"ee3e51ac-1b39-304d-a536-7fb0f42d7d9d\",\"4e3c01a1-4f61-3329-b843-edd72eaece63\",\n",
    "                \"5029d4bf-3c86-3b51-9cc2-a6c00ffd0da8\",\"df20e1e4-426e-3e59-b248-b35afd5d3bc0\",\"9dbde925-17fb-3fb6-8611-359f33cf5617\",\n",
    "                \"bfb5ea2e-96c7-3c57-b1f8-fff8c24d53f5\",\"e1eb1403-0336-3911-bb32-ef6792687a6a\",\"e7464ef1-f386-351f-b769-f8c4cd48f5b3\",\n",
    "                \"3041d19d-7433-30eb-a89b-63112ab0c83d\",\"188b9c23-de18-3e07-bee4-a66e0f6e0584\",\"b5e24993-a0d4-31aa-8568-62d86ee8be8f\",\"5f2660d1-6ef3-3f9f-a05e-8ea000937fa7\",\n",
    "                \"e3cee9b9-0b94-3603-8d75-06796eea58c4\",\"58ff314d-c43f-3c34-a2ba-f08e553b6b29\",\"b7d108bc-3ad7-37b2-a67e-f4b082e6bcb4\",\n",
    "                \"f9686201-ec1e-31ce-82ec-b416ae0ad3db\",\"8d855d72-f8c7-38be-a7e1-b28379369f6a\",\"451f02d6-9141-398d-a485-e2a2e61c8de7\",\n",
    "                \"97323ce8-c719-3780-a320-177c6db3d7e0\",\"cf241410-8fde-3155-b0ea-ee17d1f56d23\",\"b5950133-8326-3cd1-824b-40154e21a192\",\"e7118af2-14a0-39f7-a373-c97235433c9e\",\n",
    "                \"bc9bb6a1-f477-3ce7-81d8-922e282e5fa3\",\"d9772d44-04d7-3764-8470-e1dabc18377a\",\"b1ffb99c-f002-36c8-988b-a4f893528e10\",\n",
    "                 \"6600cd88-9ba5-3221-a7b5-9003ae1b2274\",\"a3fbace7-9cf3-38c8-aac2-91a4e2fdc080\",\"6600cd88-9ba5-3221-a7b5-9003ae1b2274\",\"4abde35e-5a5b-3c82-adc7-76652c59aea6\",\n",
    "                \"4bde8524-317e-3db0-a5da-877f7bf1ec66\",\"3875191b-8de8-3e01-ab1e-b3a33aab6cab\",\"3875191b-8de8-3e01-ab1e-b3a33aab6cab\",\"c60c926b-9db4-33b6-9db4-d1995a43172a\",\n",
    "                \"7811ce41-9313-394e-9de0-5c7f315d2540\",\"17070770-d8cc-336d-82e1-16bd8bfd7f2e\",\"d02fd5fe-1273-36c6-b693-16de569715b3\",\"243d577e-77cc-3926-8072-8a74f400ff0b\",\n",
    "                \"8b0a8de6-1365-34ff-a2e6-7709f8ca79d2\",\"32b5a4eb-a8a6-3a08-8a69-7d90d9e68bb5\",\"3bb25372-eb6d-39db-ab48-b2cbb9a00855\",\"6a70d9bf-f9bd-3520-903b-c2936fdd07d6\",\n",
    "                \"04e506bb-8ed3-39fc-9c88-1da66c0d5383\",\"ca6c580a-7318-35b1-8533-5e095c65c6a6\",\"4cb40796-aefe-3947-b6b4-a3b389d6a621\",\"63786a05-16ee-381b-bbe3-9a116d18cbc3\",\n",
    "                 \"9a0f90f7-6a02-35cd-9fb1-79e928ff5230\", \"9cb7335c-9347-3fef-92d4-055051c894bb\",\"d4ab707e-60c5-31ff-ade1-d68accb660fe\",\"c31b3e29-a5f7-3f5d-83b5-cda83a5d4e18\",\n",
    "                 \"3565d5dd-65fb-3860-99af-0287fa6dbab4\", \"58071ffe-1e83-3478-b651-98b990209cc7\",\"f7fd816d-f77c-31b9-ba50-33db145f0495\",\"f3f3bf62-1acf-3013-b68a-597315444b67\",\n",
    "                 \"b41f668d-2af6-31e9-9e7a-5810a79c7ece\",\"07add2ec-297f-38d5-8907-6e68885d1288\",\"b0a58353-edf9-3213-9bd5-808e7ad42bd5\",\"0a7c8426-7639-3d40-a87f-5c2078ea7393\",\n",
    "                \"038c37a8-1a6c-338a-8e8b-9f7f4f273f98\",\"fe27da79-fffb-3696-8b74-5d67f9ab49ee\",\"bbc31478-1528-34e1-9607-686c422d0378\",\"b8141e74-1083-37c5-bf7c-9d18aa932598\",\n",
    "                \"6f89e475-c5fc-39f3-bceb-a4140522bda7\",\"db1022b7-d1f4-3e6e-bd99-a9da58021321\",\"d2528038-fb55-3123-8bae-f54f09633190\",\"9c02c6b4-45f4-3d54-8497-ea2d9a9fd2a7\",\n",
    "                \"c7ea030a-0a56-3953-b2bb-4a73e5accced\"]\n",
    "\n",
    "\n",
    "\n",
    "all_completed_users=[\"5029590e-e207-3777-97e7-14041e7da291\",\n",
    "                 \"10232465-a2ea-3fb6-9726-660d0db3049e\",\"310111a3-ed04-35d8-b3d3-6cf461629ebb\",\n",
    "                \"4f8634bc-7d63-34a8-ad88-f395bb5b894e\",\"d0888fec-3787-37b0-b524-2032022bc357\",               \n",
    "                 \"c123b0f3-a897-38af-b44e-d16cee589f7f\",\"db66c264-3d1e-3b86-870e-ecc5744540e9\",\"184b34ae-8ff1-3cea-8fc0-81b52ad7d1f2\",\n",
    "                 \"c816ce9b-e10c-3c37-993e-476969d20c54\",\"5bc55063-ff9c-37d6-a6ce-ad10ed5cc5e6\",\"e2b14744-07e7-387d-a744-0190b7f5747c\",\n",
    "                    \"1fb59928-80d2-32b5-ac3d-fd3c1c411814\",\"34cfa137-a49d-3502-9ecc-6c5416a04bad\",\n",
    "                     \"0fe8c531-88ab-3b21-a455-7d1f353d5d81\",\"ce0bdd0a-53e3-38f4-999d-bac779c502a6\",\n",
    "                    \"83e7a95b-3c9f-3628-aab2-d0d1589484ce\",\"e14d8541-0130-3865-9350-d7e0e002bee5\",\"df99afe8-5ef1-392e-a706-dc90baab4575\",\"8ecb764d-864f-33ec-886f-0041c25b6195\",\n",
    "                    \"b159e569-f90e-3402-8a05-d1f76058e758\",\"e2e34bc7-0af2-3a0f-98f6-479cceab8a4e\",\"78e0c71b-9fe6-315b-a8a1-8134c6f1e306\",\n",
    "                     \"33c555b5-1037-3de1-a7c1-2362efa4b652\",\"4e8b5300-ce5a-39f4-80b3-4cd2b47dc980\",\n",
    "                     \"555892f4-7fda-37a4-bded-115ff18d1c0c\",\"7f653170-61b8-35b0-a7b8-91ccca983450\",\"c2814f41-a326-3230-96ad-c310debd89f0\",\n",
    "                     \"80e05c14-5663-34b5-b95d-9577086f808b\",\"64eaefaf-1fcc-30c5-9abe-bd8aea0e0ce3\",\"62c32dbd-a32d-3ecd-a9f1-fb9bc40fff66\",\"7b0d888e-2bcc-3a26-b64a-7b56347d89d5\",\n",
    "                    \"b442f2bd-ba45-34de-af57-d978fe41595c\",\"b95e4bdb-0ef7-3c87-87ca-3ff1e088fd1d\",\"d0b7f936-f229-3fd0-8969-f4d67fc4c90d\",\"eeb37b2e-c3de-3e31-9371-267f398e4739\",\n",
    "                    \"61af575e-7d56-35fe-a3b4-30f44719a068\",\"39edaf2c-6f84-3579-b323-d3a4f590cd04\",\"ccfc63f6-6be6-347a-8249-b9d58b97c4e9\",\"04745e04-dbfc-397a-9ca2-f626dd354d8b\",\n",
    "                    \"6ec5d54c-8b25-38f1-b075-dae157acd996\",\"da8201f9-8fea-3ba0-9481-c41f0abbc11c\",\"aacbc88a-f11a-3a35-b8a3-b28a5068318f\",\"71e88740-be67-382a-a21f-78fba469cb13\",\n",
    "                    \"ff54abe7-a4dd-3c10-a00d-e9bcc68ee92b\",\"d8191924-63ce-3857-8046-3c86c791591e\",\"835c291b-ecf4-32da-9a58-67bfe5ecfb7f\",\"48e28745-ec8f-341c-8a47-8b071a5b3e2e\",\n",
    "                    \"2f91e99e-87cf-355c-92d6-d279f1969d75\",\"bd6bf13a-0cc5-3455-a898-32d5b63ad1d2\",\"87d7d705-1cdf-36ae-9554-81afff640543\",\"ab27dc89-66d9-38eb-9062-005dae1d9686\",\n",
    "                    \"eb10e129-c551-34b8-8566-0508ccc12ae0\",\"42987d6d-47f7-3ec8-920b-b6fc6eb6e808\",\"f7400971-a4b7-326a-8120-2e1db7f91cca\",\n",
    "                     \"75f95161-ae52-3882-9cc9-64fb07a6e149\",\"59ed488e-48a4-3fd6-9ba0-5a882420f954\",\"f3590e67-0e50-31a7-ac4d-5ff1fbecb07a\",\n",
    "                    \"11e4b027-5b46-3b49-83ea-7274daeec6e7\",\"00222a15-7274-34b7-990a-81ac7d742220\",\"4bdb6f90-2048-3533-b441-a79a149b09f5\",\"8873e704-fed2-3ca1-8ac0-2001faee5a8a\",\n",
    "                    \"9591c0ed-b109-3c0b-8c5c-786cd3006294\",\"3cc71999-ed6f-3015-addb-df4554b9a86e\",\"1eb7beac-8502-3683-a430-80f7e00644c3\",\"07c8b674-2c13-3a33-a973-cf1cab70d9f9\",\n",
    "                     \"1c12f0fc-7185-380d-b62c-0b0ea34306e6\",\"462b182d-a190-3bac-9ee2-6cd88f016a61\",\"544260b1-a391-3875-bfc1-75231b2745ce\",\"7e86716e-ad5e-366d-b475-92748a1a38dc\",\n",
    "                     \"29b20b7b-876c-3958-8984-2666626a9c0c\",\"6a39888d-52b8-3be8-8bf8-19d1732a7924\",\"0f198c45-2ebb-356d-a448-5e509f2c9e13\",\"cff2761c-964c-3109-9657-684e80dd4e47\",\n",
    "                    \"b04b8870-2c49-3a06-aedf-246cc06e55f2\",\"9475c6b4-fdbe-3dd1-bf85-10943a75593b\",\"9fad8277-158b-344a-8e7c-63fc975d8558\",\"fbab085c-f318-3f35-ad34-7ec60edd3398\",\n",
    "                     \"8f8431be-3073-3aba-a1c1-d5291e3a8724\",\"38e0c362-4233-38a4-8693-81e0930f62b4\",\"d8404d54-51d1-35fc-9bf7-32aa5942c575\",\"c951b3bc-0d1c-366d-a6cf-d8b509ce291f\",\"36844d52-47aa-3910-a29d-7b56cb5e3139\",\n",
    "                    \"e37d7a22-6225-3b2c-972b-bad6572af1bb\",\"ce05c88b-b3c2-387e-afbb-bc977fd886d4\",\"e33da7f5-9dfd-3233-b09a-e5ba7a0f6dfb\",\"1553c614-8d53-34f8-86d8-2379ab75f0c4\",\"9946618c-8ca1-3740-a6b6-15cbf89b69c3\",\n",
    "                    \"39cf9343-b715-33bb-b7b5-6a610d826dfa\",\"5bb8535b-0c3d-3370-b026-e29abb38e697\",\"5bb8535b-0c3d-3370-b026-e29abb38e697\"]\n",
    "# \"d8191924-63ce-3857-8046-3c86c791591e\" - crystal already completed but wants to continue participating\n",
    "\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/only_test_users.pickle', 'wb') as handle:\n",
    "        pickle.dump(test_users, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# duplicate_users=[\"49b78324-abbd-3139-9bb1-a4397afba316\",\"4e3c01a1-4f61-3329-b843-edd72eaece63\",\n",
    "#                  \"37686d8b-d47b-33e9-99b1-496196e7ada2\",\"71e88740-be67-382a-a21f-78fba469cb13\"]\n",
    "# test_users.extend(duplicate_users)\n",
    "\n",
    "test_users.extend(pilot_users)\n",
    "\n",
    "\n",
    "email_substitutions={\"8s6gsps88m@privaterelay.appleid.com\":\"keaunnahailey@ymail.com\"    ,\n",
    "                     \"zyc6p6k5f6@privaterelay.appleid.com\":\"mtsolis@memphis.edu\",\n",
    "                     \"kz2jy7frkb@privaterelay.appleid.com\":\"quadrat.yusuph@memphis.edu\",\n",
    "                    \"2d8ptmtqmd@privaterelay.appleid.com\":\"jajhns10@memphis.edu\",\n",
    "                     \"8b6yqvccqf@privaterelay.appleid.com\":\"sarah.biggers@youthvillages.org\",\n",
    "                      \"xrnh9y4nbz@privaterelay.appleid.com\":\"brotra3@aol.com\",\n",
    "                     \"n94m4n4xrx@privaterelay.appleid.com\":\"ljackson8@live.com\",\n",
    "                     \"psrh94mjqv@privaterelay.appleid.com\":\"conniewmills@gmail.com\",\n",
    "                     \"2kqk57y9vb@privaterelay.appleid.com\":\"dptrson2@memphis.edu\",                     \n",
    " \"6qprqrc2pb@privaterelay.appleid.com\":\"eadderson@bellsouth.net\",\n",
    "                     \"w9zcdbzdm9@privaterelay.appleid.com\":\"vegterp@apsu.edu\"\n",
    "}\n",
    "uuid_substitutions={\"4e3c01a1-4f61-3329-b843-edd72eaece63\":\"62c32dbd-a32d-3ecd-a9f1-fb9bc40fff66\",\n",
    "                   \"37686d8b-d47b-33e9-99b1-496196e7ada2\":\"ff54abe7-a4dd-3c10-a00d-e9bcc68ee92b\",\n",
    "                   \"68d89413-5a7f-3a3b-9e6c-b22d2699307a\":\"07c8b674-2c13-3a33-a973-cf1cab70d9f9\",\n",
    "                   \"7e0aa5f7-96cd-3a95-a28d-e3ef3684e0e1\":\"71e88740-be67-382a-a21f-78fba469cb13\",\n",
    "                   \"559c51b0-672e-32d9-a1d7-f7b36b5190af\":\"f7400971-a4b7-326a-8120-2e1db7f91cca\",\n",
    "                   \"b9b13911-dda5-38de-bb0d-becd0c9a4c7d\":\"835c291b-ecf4-32da-9a58-67bfe5ecfb7f\",\n",
    "                   \"fd1574fe-3093-3519-949e-15c98b4bc73a\":\"d8404d54-51d1-35fc-9bf7-32aa5942c575\",\n",
    "                   \"336482c0-71cf-39c8-96bb-8aeb7ffa4d3f\":\"b0a58353-edf9-3213-9bd5-808e7ad42bd5\",\n",
    "                   \"1c7ddb41-c249-3894-9843-8b58a9657d79\":\"e33da7f5-9dfd-3233-b09a-e5ba7a0f6dfb\",\n",
    "                    \"68694195-879e-35ee-ab15-4598e3b4a1dc\":\"36844d52-47aa-3910-a29d-7b56cb5e3139\"}\n",
    "# 37686d8b-d47b-33e9-99b1-496196e7ada2\":\"ff54abe7-a4dd-3c10-a00d-e9bcc68ee92b\n",
    "\n",
    "def uuid_substitute(user):\n",
    "    if user in uuid_substitutions:        \n",
    "        return uuid_substitutions[user]\n",
    "    else:\n",
    "        return user\n",
    "\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/test_users.pickle', 'wb') as handle:\n",
    "        pickle.dump(test_users, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/withdrawn_users.pickle', 'wb') as handle:\n",
    "        pickle.dump(withdrawn_users, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/pilot_users.pickle', 'wb') as handle:\n",
    "        pickle.dump(pilot_users, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/all_completed_users.pickle', 'wb') as handle:\n",
    "        pickle.dump(all_completed_users, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 79, 23)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_completed_users),len(withdrawn_users),len(pilot_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/cerebralcortex/core/data_manager/raw/filebased_storage.py:179: UserWarning: Setting version='all'  is experimental If multiple versions of a stream have different schemas then any operation on datastream will throw schema mismatch exception.\n",
      "  warnings.warn(\"Setting version='all'  is experimental If multiple versions of a stream have different schemas then any operation on datastream will throw schema mismatch exception.\")\n"
     ]
    }
   ],
   "source": [
    "from cerebralcortex import Kernel\n",
    "\n",
    "date_list=[]\n",
    "date_list.append(today)\n",
    "for i in range(7):\n",
    "    date_list.append(today - datetime.timedelta(days=i+1))\n",
    "    \n",
    "\n",
    "\n",
    "def uuid_substitute(user):\n",
    "    if user in uuid_substitutions:        \n",
    "        return uuid_substitutions[user]\n",
    "    else:\n",
    "        return user\n",
    "CC = Kernel(\"/home/jupyter/cc3_moods_conf/\", study_name='moods')\n",
    "mood_epis=CC.get_stream(\"org.md2k.moods.episodes\",version=\"all\")\n",
    "mood_epis=mood_epis.toPandas()\n",
    "mood_epis[\"user\"]=mood_epis[\"user\"].apply(uuid_substitute)\n",
    "mood_epis[\"value\"]=round(mood_epis[\"value\"],5)\n",
    "mood_epis['date'] = mood_epis['starttime'].dt.date\n",
    "mood_epis=mood_epis.loc[mood_epis[\"date\"].isin(date_list)]\n",
    "mood_epis=mood_epis.loc[mood_epis['endtime'].shift() != mood_epis['endtime']]\n",
    "mood_epis['duration'] = mood_epis['endtime'].sub(mood_epis['starttime'], axis=0)\n",
    "mood_epis['duration'] = mood_epis['duration'] / np.timedelta64(1, 'm')\n",
    "mood_epis_self=mood_epis.loc[mood_epis[\"user_generated\"]==1]\n",
    "mood_epis=mood_epis.loc[mood_epis[\"user_generated\"]==0]\n",
    "\n",
    "\n",
    "in_active_complete_users=[]\n",
    "for user  in all_completed_users: \n",
    "    mood_epis_user=mood_epis.loc[mood_epis[\"user\"]==user].sort_values([\"starttime\"],ascending=False).drop_duplicates()   \n",
    "    user_selected=mood_epis_user.loc[mood_epis_user[\"selected\"]==1]\n",
    "    user_annotated=user_selected.loc[user_selected[\"user_rating\"].notnull()]\n",
    "    if len(user_annotated) == 0:          \n",
    "        in_active_complete_users.append(user) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in_active_complete_users,\n",
    "len(pilot_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_active_complete_users\n",
    "\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/in_active_complete_users.pickle', 'wb') as handle:\n",
    "        pickle.dump(in_active_complete_users, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2024, 1, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encrypting the name and email\n",
    "\n",
    "from cryptography.fernet import Fernet\n",
    "key = Fernet.generate_key()\n",
    "  \n",
    "# Instance the Fernet class with the key\n",
    "  \n",
    "fernet = Fernet(key)\n",
    "\n",
    "out_file = open(\"/home/jupyter/sneupane/MOODS/deployment/key_file\", \"wb\") # open for [w]riting as [b]inary\n",
    "out_file.write(key)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_date(x):\n",
    "    if x is None:\n",
    "        return \"No start date\"\n",
    "    else:\n",
    "        return str(x.date())\n",
    "\n",
    "def start_time_date(x):    \n",
    "    if x is None:\n",
    "        return  \"No start date\"\n",
    "    else:\n",
    "        return (datetime.datetime.fromtimestamp(float(x)/1000.0).date())\n",
    "    \n",
    "    \n",
    "\n",
    "def next_weekday(d, weekday):\n",
    "    days_ahead = weekday - d.weekday()\n",
    "    if days_ahead <= 0: # Target day already happened this week\n",
    "        days_ahead += 7\n",
    "    return d + datetime.timedelta(days_ahead)\n",
    "\n",
    "\n",
    "def read_mood_user_stream():\n",
    "    \n",
    "    CC = Kernel(\"/home/jupyter/cc3_moods_conf/\", study_name='moods')\n",
    "    udf_date = F.udf(datetime_date,StringType())\n",
    "    user_list_stream=CC.get_stream(\"org.md2k.moods.userlist\").withColumn(\"registered_date\", udf_date('registered_on'))\n",
    "    user_list=[x.user for x in user_list_stream.select('user').distinct().collect()]\n",
    "    return user_list_stream,list(set(user_list))\n",
    "\n",
    "def week_mapping(user,start_date,first_week):\n",
    "    week_dict={}\n",
    "    if first_week !=0:\n",
    "        if (first_week-start_date).days > 6:\n",
    "            counter=1\n",
    "            temp_dic_week={}\n",
    "            temp_dic_count={}\n",
    "            end_date= date.today()\n",
    "            delta = end_date - (first_week +timedelta(days=1))\n",
    "            delta_firstweek = first_week - start_date\n",
    "\n",
    "            temp_days=[]\n",
    "            temp_count=[]\n",
    "            temp_weeks=[]\n",
    "            for i in range(delta_firstweek.days + 1):\n",
    "                day = start_date + timedelta(days=i)\n",
    "                temp_days.append(day)\n",
    "                temp_weeks.append(1)\n",
    "                temp_count.append(counter)\n",
    "                counter+=1\n",
    "            counter1=8\n",
    "            \n",
    "            for i in range(delta.days + 1):\n",
    "                day = first_week +timedelta(days=1) + timedelta(days=i)\n",
    "                temp_days.append(day)\n",
    "                temp_weeks.append(math.ceil(counter1/7))\n",
    "                temp_count.append(counter)\n",
    "                counter+=1\n",
    "                counter1+=1\n",
    "\n",
    "            temp_dic_week = dict(zip(temp_days, temp_weeks))\n",
    "            temp_dic_count = dict(zip(temp_days, temp_count))\n",
    "\n",
    "            week_dict[\"Weeks_days\"]=temp_dic_week \n",
    "            week_dict[\"Count_days\"]=temp_dic_count \n",
    "        else:\n",
    "            temp_dic_week={}\n",
    "            temp_dic_count={}\n",
    "            end_date=date.today()\n",
    "            delta = end_date - start_date\n",
    "            counter=1\n",
    "            temp_days=[]\n",
    "            temp_weeks=[]\n",
    "            temp_count=[]\n",
    "            for i in range(delta.days + 1):\n",
    "                day = start_date + timedelta(days=i)\n",
    "                temp_days.append(day)\n",
    "                temp_weeks.append(math.ceil(counter/7))\n",
    "                temp_count.append(counter)\n",
    "                counter+=1\n",
    "            temp_dic_week = dict(zip(temp_days, temp_weeks))\n",
    "            temp_dic_count = dict(zip(temp_days, temp_count))\n",
    "\n",
    "            week_dict[\"Weeks_days\"]=temp_dic_week \n",
    "            week_dict[\"Count_days\"]=temp_dic_count \n",
    "#         print(week_dict)\n",
    "        return week_dict\n",
    "            \n",
    "    \n",
    "def create_user_profile(user_list_stream_pdf,user_list):\n",
    "#     today = date.today()\n",
    "  \n",
    "#     user_list_stream_pdf[\"study_start_date\"]=user_list_stream_pdf[\"start_time\"].map(start_time_date)\n",
    "#     user_list_stream_pdf=user_list_stream_pdf.loc[user_list_stream_pdf[\"study_start_date\"]!=\"No start date\"]\n",
    "#     user_list_stream_pdf=user_list_stream_pdf.loc[user_list_stream_pdf[\"study_start_date\"]<=today]\n",
    "    consent_fields=[]\n",
    "    user_profile={}\n",
    "    user_days_week={}\n",
    "    for user in user_list:\n",
    "#         if user not in withdrawn_users:\n",
    "            temp_df_pandas=user_list_stream_pdf[user_list_stream_pdf.user==user]\n",
    "            temp_consent=temp_df_pandas[\"consent_document\"].iloc[0]\n",
    "            joined_date=temp_df_pandas[\"registered_date\"].iloc[0]\n",
    "\n",
    "            study_start_date=temp_df_pandas[\"study_start_date\"].iloc[0]\n",
    "            temp_dict=json.loads(temp_consent)\n",
    "            consent_fields=list(temp_dict.keys())\n",
    "            temp_profile=[]       \n",
    "            temp_profile_2=[]\n",
    "\n",
    "            total_days=0\n",
    "            curr_week=0\n",
    "            week_one=1\n",
    "            first_week=0\n",
    "            next_friday=0\n",
    "#             print(today)\n",
    "            if (study_start_date) !=\"No start date\":\n",
    "        \n",
    "                if  calendar.day_name[study_start_date.weekday()] == \"Saturday\" :\n",
    "                    next_friday=next_weekday(study_start_date, 4)\n",
    "                    first_week=next_friday\n",
    "                    total_days=np.busday_count(study_start_date, today,\"1111111\")+1  \n",
    "    #                 first_week=study_start_date\n",
    "                    curr_week=int(math.ceil(total_days/7))\n",
    "    #                 pass\n",
    "                elif calendar.day_name[study_start_date.weekday()] == \"Friday\" :   \n",
    "                    next_friday=next_weekday(study_start_date, 4)\n",
    "                    first_week=next_friday\n",
    "                    total_days=np.busday_count(study_start_date, today,\"1111111\")+1\n",
    "                    modified_total_days=np.busday_count(next_friday-timedelta(6), today,\"1111111\")+1\n",
    "                    curr_week=int(math.ceil(modified_total_days/7))\n",
    "\n",
    "\n",
    "                else:\n",
    "                    next_friday=next_weekday(next_weekday(study_start_date, 4), 4)               \n",
    "\n",
    "                    first_week=next_friday\n",
    "                    total_days=np.busday_count(study_start_date, today,\"1111111\")+1\n",
    "                    modified_total_days=np.busday_count(next_friday-timedelta(6), today,\"1111111\")+1  \n",
    "                    curr_week=int(math.ceil(modified_total_days/7))\n",
    "                    if curr_week ==0:\n",
    "                        curr_week=1\n",
    "\n",
    "            else:\n",
    "                total_days=\"NA\"\n",
    "                curr_week=\"NA\"       \n",
    "\n",
    "    #         print(i,temp_dict[\"name\"])\n",
    "\n",
    "            # as of now i am storing only those users who have completed the total consent documents\n",
    "            if len (consent_fields) >= 13  :\n",
    "\n",
    "\n",
    "                temp_profile.append(fernet.encrypt(temp_dict[\"name\"].encode()))\n",
    "#                 temp_profile.append(temp_dict[\"name\"])\n",
    "                if temp_dict[\"email\"] in email_substitutions:\n",
    "                    temp_profile.append(fernet.encrypt(email_substitutions[temp_dict[\"email\"]].encode()))\n",
    "                else:\n",
    "                    temp_profile.append(fernet.encrypt(temp_dict[\"email\"].encode()))\n",
    "                temp_profile.append(study_start_date)\n",
    "                temp_profile.append(first_week)\n",
    "                temp_profile.append(total_days)\n",
    "                temp_profile.append(curr_week)\n",
    "                temp_profile_2.append(week_mapping(user,study_start_date,first_week))\n",
    "\n",
    "                user_profile[user]=temp_profile           \n",
    "                user_days_week[user]=temp_profile_2\n",
    "    return user_profile,user_days_week\n",
    "\n",
    "\n",
    "def user_profile_create(user_profile,user_days_week,flag):    \n",
    "\n",
    "    if flag==False:\n",
    "\n",
    "        with open('/home/jupyter/sneupane/MOODS/pickled_files/user_profile.pickle', 'wb') as handle:\n",
    "            pickle.dump(user_profile, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open('/home/jupyter/sneupane/MOODS/pickled_files/user_days_week.pickle', 'wb') as handle:\n",
    "            pickle.dump(user_days_week, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open('/home/jupyter/sneupane/MOODS/analysis/papers/Ubicomp/dataframe/user_days_week.pickle', 'wb') as handle:\n",
    "            pickle.dump(user_days_week, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('/home/jupyter/sneupane/MOODS/pickled_files/user_profile.pickle', 'wb') as handle:\n",
    "            pickle.dump(user_profile, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open('/home/jupyter/sneupane/MOODS/pickled_files/user_days_week.pickle', 'wb') as handle:\n",
    "            pickle.dump(user_days_week, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        \n",
    "user_list_stream,user_list=read_mood_user_stream()\n",
    "user_list_stream_pdf=user_list_stream.toPandas()\n",
    "user_list_stream_pdf[\"study_start_date\"]=user_list_stream_pdf[\"start_time\"].map(start_time_date)\n",
    "user_list_stream_pdf=user_list_stream_pdf.loc[user_list_stream_pdf[\"study_start_date\"]!=\"No start date\"]\n",
    "user_list_stream_pdf=user_list_stream_pdf.loc[user_list_stream_pdf[\"study_start_date\"]<=today]\n",
    "# user_list_stream_pdf[\"user\"]=user_list_stream_pdf[\"user\"].apply(uuid_substitute)\n",
    "user_list=user_list_stream_pdf.user.unique().tolist()\n",
    "user_list_stream_pdf=user_list_stream_pdf.loc[user_list_stream_pdf[\"user\"]!=\"b9588ec7-2376-3f9a-bcab-bd91fbf84a23\"]\n",
    "if \"b9588ec7-2376-3f9a-bcab-bd91fbf84a23\" in user_list:\n",
    "    user_list.remove(\"b9588ec7-2376-3f9a-bcab-bd91fbf84a23\")\n",
    "user_profile,user_days_week=create_user_profile(user_list_stream_pdf,user_list)\n",
    "user_profile_create(user_profile,user_days_week,flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_days_week[\"fbab085c-f318-3f35-ad34-7ec60edd3398\"]\n",
    "# today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC3 done_completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"CC3 done_completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickled_files():   \n",
    "\n",
    "\n",
    "    with open('/home/jupyter/sneupane/MOODS/pickled_files/user_profile.pickle', 'rb') as handle:\n",
    "        user_profile = pickle.load(handle)\n",
    "        \n",
    "    return user_profile\n",
    "\n",
    "def next_weekday(d, weekday):\n",
    "    days_ahead = weekday - d.weekday()\n",
    "    if days_ahead <= 0: # Target day already happened this week\n",
    "        days_ahead += 7\n",
    "    return d + timedelta(days_ahead)\n",
    "\n",
    "\n",
    "def start_time_date(x):    \n",
    "    if x is None:\n",
    "        return  \"No start date\"\n",
    "    else:\n",
    "        return (datetime.datetime.fromtimestamp(float(x)/1000.0).date())\n",
    "\n",
    "def create_final_dataframe(user_list):\n",
    "#     today= date.today()\n",
    "    \n",
    "    CC = Kernel(\"/home/jupyter/cc3_moods_conf/\", study_name='moods')\n",
    "    user_list_stream=CC.get_stream(\"org.md2k.moods.userlist\")\n",
    "    user_list_stream_pdf=user_list_stream.toPandas()\n",
    "    user_list_stream_pdf=user_list_stream_pdf.loc[user_list_stream_pdf[\"user\"]!=\"b9588ec7-2376-3f9a-bcab-bd91fbf84a23\"]\n",
    "    user_list_stream_pdf[\"study_start_date\"]=user_list_stream_pdf[\"start_time\"].map(start_time_date)\n",
    "    user_list_stream_pdf=user_list_stream_pdf[[\"user\",\"registered_on\",\"study_start_date\"]]\n",
    "    appended_data =[]\n",
    "    \n",
    "    for user in user_list:\n",
    "        \n",
    "        user_total_days={}\n",
    "        temp_dic_day={}\n",
    "        temp_dic_week={}\n",
    "        temp_df=user_list_stream_pdf[user_list_stream_pdf.user==user]\n",
    "        start_date=temp_df[\"study_start_date\"].iloc[0]\n",
    "        if (start_date) !=\"No start date\":\n",
    "            # get the total days of study till the current date \n",
    "            if  calendar.day_name[start_date.weekday()] == \"Saturday\" :\n",
    "                  \n",
    "                total_days=np.busday_count(temp_df['study_start_date'].iloc[0], today,\"1111111\")+1\n",
    "                # store the first day and total days in a dict\n",
    "                user_total_days[user]=[temp_df['study_start_date'].iloc[0], today,total_days]         \n",
    "                temp_df=temp_df.sort_values(\"study_start_date\")\n",
    "                start_date=temp_df['study_start_date'].iloc[0]   \n",
    "                next_friday=next_weekday(start_date, 4)\n",
    "                temp_df[\"counter\"]=total_days\n",
    "                temp_df[\"start_date\"]=start_date\n",
    "                temp_df[\"first_week\"]=next_friday\n",
    "                temp_df[\"week\"]=math.ceil(total_days/7)\n",
    "                appended_data.append(temp_df)\n",
    "#                 pass\n",
    "            elif calendar.day_name[start_date.weekday()] == \"Friday\" :\n",
    "                start_date=temp_df['study_start_date'].iloc[0]\n",
    "                next_friday=next_weekday(start_date, 4)\n",
    "                total_days=np.busday_count(start_date, today,\"1111111\")+1\n",
    "                modified_total_days=np.busday_count(next_friday-timedelta(6), today,\"1111111\")+1\n",
    "#                 curr_week=math.ceil(modified_total_days/7)\n",
    "                \n",
    "                # store the first day and total days in a dict\n",
    "                user_total_days[user]=[temp_df['study_start_date'].iloc[0], today,total_days]         \n",
    "                temp_df=temp_df.sort_values(\"study_start_date\")\n",
    "                start_date=temp_df['study_start_date'].iloc[0]     \n",
    "                temp_df[\"counter\"]=total_days\n",
    "                temp_df[\"start_date\"]=start_date\n",
    "                temp_df[\"first_week\"]=next_friday\n",
    "#                 temp_df[\"modified_counter\"]=total_days\n",
    "                temp_df[\"week\"]=math.ceil(modified_total_days/7)\n",
    "                appended_data.append(temp_df)\n",
    "#                 pass\n",
    "\n",
    "\n",
    "            else:            \n",
    "                              \n",
    "                start_date=temp_df['study_start_date'].iloc[0]\n",
    "                next_friday=next_weekday(next_weekday(start_date, 4), 4)\n",
    "                total_days=np.busday_count(start_date, today,\"1111111\")+1\n",
    "                modified_total_days=np.busday_count(next_friday-timedelta(6), today,\"1111111\")+1\n",
    "\n",
    "                \n",
    "                # store the first day and total days in a dict\n",
    "                user_total_days[user]=[temp_df['study_start_date'].iloc[0], today,total_days]         \n",
    "                temp_df=temp_df.sort_values(\"study_start_date\")   \n",
    "                temp_df[\"counter\"]=total_days\n",
    "                temp_df[\"start_date\"]=start_date\n",
    "                temp_df[\"first_week\"]=next_friday\n",
    "#                 temp_df[\"modified_counter\"]=total_days\n",
    "                temp_df[\"week\"]=math.ceil(modified_total_days/7)\n",
    "                appended_data.append(temp_df)\n",
    "  \n",
    "    final_df = pd.concat(appended_data)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def create_user_dates_csv(user_df_counter,user_profile):\n",
    "    final_df1=pd.DataFrame()\n",
    "    for user in user_profile:\n",
    "        user_vis_week={}       \n",
    "        temp_df=user_df_counter.loc[user_df_counter[\"user\"]==user]\n",
    "        if len(temp_df)>0:\n",
    "            first_week=temp_df[\"first_week\"].iloc[0]\n",
    "            start_day=temp_df[\"start_date\"].iloc[0]\n",
    "            appended_data=[]\n",
    "            appended_data.append([user,start_day,1,str(first_week),str(first_week+timedelta(days=1)),str(first_week+timedelta(days=2))])\n",
    "            for week in range(1,50):                \n",
    "                appended_data.append([user,start_day,week+1,str(first_week+timedelta(days=7*week)),str(first_week+timedelta(days=7*week+1)),str(first_week+timedelta(days=7*week+2))])\n",
    "            df = pd.DataFrame(appended_data)\n",
    "            final_df1 = pd.concat([final_df1,df])\n",
    "    final_df1.columns=[\"user\",\"start_date\",\"Week\",\"creation_date\",\"sanity_date\",\"delivery_date\"]\n",
    "    return final_df1\n",
    "\n",
    "\n",
    "user_profile=read_pickled_files()\n",
    "user_df_counter=create_final_dataframe(user_profile)\n",
    "final_df1=create_user_dates_csv(user_df_counter,user_profile)\n",
    "final_df1.to_csv(\"/home/jupyter/sneupane/MOODS/csv_files/user_dates.csv\",index=False)\n",
    "\n",
    "\n",
    "import pickle\n",
    "from datetime import date,datetime\n",
    "import pandas as pd\n",
    "def read_pickled_files():   \n",
    "\n",
    "\n",
    "    with open('/home/jupyter/sneupane/MOODS/pickled_files/user_profile.pickle', 'rb') as handle:\n",
    "        user_profile = pickle.load(handle)\n",
    "    user_dates=pd.read_csv(\"/home/jupyter/sneupane/MOODS/csv_files/user_dates.csv\")\n",
    "    return user_profile,user_dates\n",
    "\n",
    "\n",
    "def user_creation_date(user_dates_df,user_profile,date):\n",
    "    users=[]\n",
    "    for user in user_profile:\n",
    "        user_df=user_dates_df.loc[user_dates_df[\"user\"]==user]\n",
    "        user_df=user_df.loc[user_df['creation_date'] == date]\n",
    "        if len(user_df):\n",
    "            users.append(user_df[\"user\"].values.tolist()[0])\n",
    "                        \n",
    "    return users\n",
    "\n",
    "user_profile,user_dates_df=read_pickled_files()\n",
    "# today=date.today()\n",
    "\n",
    "users_today=user_creation_date(user_dates_df,user_profile,str(today))\n",
    "# print(users_today)\n",
    "if include_withdrawn_completed_users == False:\n",
    "    users_today=[user for user in users_today if user not in withdrawn_users]\n",
    "    users_today=[user for user in users_today if user not in in_active_complete_users]\n",
    "else:\n",
    "    pass\n",
    "    \n",
    "users_today_with_test_users=users_today\n",
    "users_today_without_test_users=[user for user in users_today if user not in test_users]\n",
    "\n",
    "# users_today_without_test_users=[\"cfe7d4d1-b4fa-3eb5-8f15-e78257e61cf3\"]\n",
    "# if test users to be included\n",
    "# with open('/home/jupyter/sneupane/MOODS/pickled_files/users_today.pickle', 'wb') as handle:\n",
    "#             pickle.dump(users_today_with_test_users, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# if test users not to be included\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/users_today.pickle', 'wb') as handle:\n",
    "            pickle.dump(users_today_without_test_users, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/users_today.pickle', 'rb') as handle:\n",
    "        users_today = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users_today)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230423 242408\n",
      "['' 'private']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import import_ipynb\n",
    "from pyecharts.charts import Pie\n",
    "from pyecharts import options as opts\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from cerebralcortex import Kernel\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructField, StructType, LongType, \\\n",
    "DoubleType,MapType, StringType,ArrayType, FloatType, TimestampType, IntegerType, DateType\n",
    "from pyspark.sql.functions import col\n",
    "from cerebralcortex.util.helper_methods import get_study_names\n",
    "\n",
    "import pandas as pd\n",
    "# %reload_ext autoreload\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import datetime as datetime_adj\n",
    "import calendar\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import date, timedelta\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "\n",
    "CC = Kernel(\"/home/jupyter/cc3_moods_conf/\", study_name='moods')\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/user_profile.pickle', 'rb') as handle:\n",
    "    user_profile = pickle.load(handle)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/user_days_week.pickle', 'rb') as handle:\n",
    "    user_days_week = pickle.load(handle)\n",
    "\n",
    "\n",
    "\n",
    "# this function maps time to time_of_day\n",
    "def time_to_time_of_day(i):\n",
    "    \n",
    "#     print(type(i))\n",
    "    tmp=\"\"\n",
    "    if  datetime_adj.time(5, 0)<=i.time() < datetime_adj.time(12, 0):\n",
    "        tmp=\"Morning\"\n",
    "    elif  datetime_adj.time(12, 0,0)<=i.time() < datetime_adj.time(17, 0,0):\n",
    "        tmp= \"Afternoon\"\n",
    "    elif  datetime_adj.time(17, 0,0)<=i.time() < datetime_adj.time(21, 0,0):\n",
    "        tmp= \"Evening\"\n",
    "    elif  datetime_adj.time(21, 0,0)<=i.time() <= datetime_adj.time(23, 59,59):\n",
    "        tmp= \"Night\"\n",
    "    elif  datetime_adj.time(0, 0,0)<=i.time() < datetime_adj.time(5, 0,0):\n",
    "        tmp= \"Night\"\n",
    "        \n",
    "    if tmp == \"\":\n",
    "        print(str(i) + \" :\" +str(i.time()))\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\n",
    "#this function maps the date to day of the week\n",
    "\n",
    "def date_to_day_short(date1):\n",
    "#     print(date1)\n",
    "\n",
    "    day_dict={\"Monday\":\"Mon\",\"Tuesday\":\"Tue\",\"Wednesday\":\"Wed\",\"Thursday\":\"Thu\",\"Friday\":\"Fri\",\"Saturday\":\"Sat\",\"Sunday\":\"Sun\"}\n",
    "    return day_dict[calendar.day_name[date1.weekday()]]\n",
    "\n",
    "def date_to_day_full(date1):\n",
    "    return calendar.day_name[date1.weekday()]\n",
    "#this function gives weekend or weekday\n",
    "\n",
    "def weekday_weeekend(day):\n",
    "    if day == \"Saturday\" or day == \"Sunday\":\n",
    "        return \"weekend\"\n",
    "    else:\n",
    "        return \"weekday\"\n",
    "    \n",
    "#this function returns only date    \n",
    "#this function returns only date    \n",
    "def datetime_date(x):\n",
    "    return (x.date())\n",
    "\n",
    "def round_to_hour(dt):\n",
    "    dt_start_of_hour = dt.replace(minute=0, second=0, microsecond=0)\n",
    "    return dt_start_of_hour\n",
    "\n",
    "def add_custom_cols(episodes_list_pdf):\n",
    "    episodes_list_pdf[\"episode_start_day\"]=episodes_list_pdf[\"starttime\"].apply(datetime_date)\n",
    "    episodes_list_pdf[\"episode_end_day\"]=episodes_list_pdf[\"endtime\"].apply(datetime_date)\n",
    "    episodes_list_pdf[\"day_full\"]=episodes_list_pdf[\"starttime\"].apply(date_to_day_full)\n",
    "    episodes_list_pdf[\"day_short\"]=episodes_list_pdf[\"starttime\"].apply(date_to_day_short)\n",
    "    episodes_list_pdf[\"weekend_weekday\"]=episodes_list_pdf[\"day_full\"].apply(weekday_weeekend)\n",
    "    episodes_list_pdf[\"start_time_of_day\"]=episodes_list_pdf[\"starttime\"].map(time_to_time_of_day)\n",
    "    episodes_list_pdf[\"end_time_of_day\"]=episodes_list_pdf[\"endtime\"].map(time_to_time_of_day)\n",
    "    episodes_list_pdf['start_day'] = pd.to_datetime(episodes_list_pdf['episode_start_day'])\n",
    "    episodes_list_pdf['end_day'] = pd.to_datetime(episodes_list_pdf['episode_end_day'])\n",
    "    episodes_list_pdf['start_hour'] = episodes_list_pdf['starttime'].apply(round_to_hour).dt.hour\n",
    "    episodes_list_pdf['end_hour'] = episodes_list_pdf['endtime'].apply(round_to_hour).dt.hour\n",
    "    episodes_list_pdf[\"days_diff\"]=(episodes_list_pdf['episode_end_day'] - episodes_list_pdf['episode_start_day']).dt.days\n",
    "    episodes_list_pdf=episodes_list_pdf.loc[episodes_list_pdf[\"days_diff\"] < 2]\n",
    "    # calculating the duration of episodes in minutes\n",
    "    episodes_list_pdf['duration'] = episodes_list_pdf['endtime'].sub(episodes_list_pdf['starttime'], axis=0)\n",
    "    episodes_list_pdf['duration'] = episodes_list_pdf['duration'] / np.timedelta64(1, 'm')\n",
    "    episodes_list_pdf[\"user_context\"] = episodes_list_pdf[\"user_context\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    episodes_list_pdf.user_context.fillna(value=\"Unknown\", inplace=True)\n",
    "    episodes_list_pdf['user_context']=episodes_list_pdf['user_context'].str.lower()\n",
    "#     episodes_list_pdf[\"flag\"]=episodes_list_pdf[\"tag_name\"].str.split(\"###\").str[1].str.split(\"=\").str[1].str.split(\" \").str[0]\n",
    "#     episodes_list_pdf[\"tag_name\"]=episodes_list_pdf[\"tag_name\"].str.split(\"###\").str[0]\n",
    "    episodes_list_pdf['tag_name']=episodes_list_pdf['tag_name'].str.lower()\n",
    "    \n",
    "#     today = date.today()\n",
    "\n",
    "    episodes_list_pdf=episodes_list_pdf.sort_values([\"user\",\"episode_start_day\"])\n",
    "    appended_data =[]\n",
    "    user_total_days={}\n",
    "    user_days={}\n",
    "    for user in user_profile:\n",
    "        temp_dic_day={}\n",
    "        temp_dic_week={}\n",
    "        \n",
    "        temp=episodes_list_pdf[episodes_list_pdf.user==user]\n",
    "        #todo\n",
    "#         if len(temp) > 0:\n",
    "        if len(temp) > 0 and user_profile[user][2] !=\"No start date\":\n",
    "            user_days=user_days_week[user]\n",
    "            \n",
    "            temp=temp.loc[temp[\"episode_start_day\"] >= user_profile[user][2] ]\n",
    "        \n",
    "#             total_days=np.busday_count(user_profile[i][2], today,\"1111111\")+1\n",
    "\n",
    "            temp[\"counter\"]=temp[\"episode_start_day\"].map(user_days[0][\"Count_days\"])\n",
    "          \n",
    "            temp[\"week\"]=temp[\"episode_start_day\"].map(user_days[0][\"Weeks_days\"])\n",
    "\n",
    "            appended_data.append(temp)\n",
    "    final_df = pd.concat(appended_data)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def create_final_dataframe():\n",
    "    \n",
    "    \n",
    "    # udf_uuid=F.udf(uuid_to_name,StringType())\n",
    "\n",
    "    episodes_list=CC.get_stream(\"org.md2k.moods.episodes\")\n",
    "    episodes_list=episodes_list.sort(col(\"starttime\").desc())   \n",
    " \n",
    "    episodes_list_pdf=episodes_list.toPandas()\n",
    "    episodes_list_pdf=episodes_list_pdf.drop_duplicates()\n",
    "    episodes_list_pdf=episodes_list_pdf.loc[episodes_list_pdf[\"stress_id\"].notnull()]\n",
    "#     episodes_list_pdf = episodes_list_pdf.loc[episodes_list_pdf['endtime'].shift() != episodes_list_pdf['endtime']]\n",
    "    # replace UUID for duplicates\n",
    "    episodes_list_pdf[\"user\"]=episodes_list_pdf[\"user\"].apply(uuid_substitute)\n",
    "    \n",
    "    # filtering data only after october 1st 2020\n",
    "\n",
    "    final_df=add_custom_cols(episodes_list_pdf)\n",
    "    print(len(final_df),len(episodes_list_pdf))\n",
    "    final_df.rename(columns={'user_context':'Location','tag_name':\"Stressor\"}, inplace=True)\n",
    "    final_df = final_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    print(final_df.state.unique())\n",
    "    # this filter outs data where users have made the episodes private\n",
    "#     final_df_not_deleted=final_df.loc[final_df.deleted==0]\n",
    "    final_df_not_deleted=final_df.loc[final_df.state!=\"private\"]\n",
    "\n",
    "    # this filters selected and user generated episodes\n",
    "    final_df_not_deleted= final_df_not_deleted.loc[final_df_not_deleted.episode_class != -1]\n",
    "    final_df_selected=final_df_not_deleted.loc[(final_df_not_deleted.selected==1) & ( final_df_not_deleted.user_generated != 1)& ( final_df_not_deleted.episode_class != -1)]\n",
    "#     final_df_selected=final_df_not_deleted.loc[(final_df_not_deleted.selected==1)]\n",
    "    final_df_not_stressed=final_df_selected.loc[final_df_selected[\"user_rating\"].isin([ 'Probably not stressed', 'Not stressed'])]\n",
    "    final_df_stressed=final_df_selected.loc[final_df_selected[\"user_rating\"].isin([\"Unsure\", \"Probably stressed\",\"Stressed\"])]\n",
    "    final_df_self=final_df_not_deleted.loc[final_df_not_deleted[\"user_generated\"]==1 | ( final_df_not_deleted.episode_class == -1)]\n",
    "#     final_df_stressed.rename(columns={'user_context':'Location','tag_name':\"Stressor\"}, inplace=True)\n",
    "    return final_df,final_df_stressed,final_df_not_stressed,final_df_self\n",
    "# this filters null tag_name data\n",
    "# final_df_tag_not_null=final_df_selected.loc[final_df_selected['tag_name'].notnull()]\n",
    "\n",
    "final_df,final_df_stressed,final_df_not_stressed,final_df_self=create_final_dataframe()\n",
    "\n",
    "\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/pre_stressed_df.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_df_stressed, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/not_stressed_df.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_df_not_stressed, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/final_df.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "if save_chi:    \n",
    "#     print(final_df.state.unique())\n",
    "    with open('/home/jupyter/sneupane/MOODS/analysis/papers/CHI/dataframe/pre_final_df.pickle', 'wb') as handle:\n",
    "        pickle.dump(final_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     with open('/home/jupyter/sneupane/MOODS/analysis/CHI/dataframe/self_df.pickle', 'wb') as handle:\n",
    "#         pickle.dump(final_df_self, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/pre_self_df.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_df_self, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a=final_df_stressed.loc[final_df_stressed[\"user\"]==\"62c32dbd-a32d-3ecd-a9f1-fb9bc40fff66\"]\n",
    "# # a\n",
    "len(final_df_self)\n",
    "# final_df_self.user_generated.unique()\n",
    "# final_df.sort_values(\"starttime\").tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230423\n",
      "231070\n",
      "231070\n",
      "286068\n"
     ]
    }
   ],
   "source": [
    "# final_df=final_df.rename(columns={\"episode_start_day\":\"date\"})\n",
    "pre_final_df_splitted=final_df[['stress_id', 'starttime', 'endtime', 'value', 'confidence',\n",
    "       'episode_class', 'deleted', 'state', 'user_generated', 'user_rating',\n",
    "       'Location', 'selected', 'latitude', 'longitude', 'radius', 'label',\n",
    "       'Stressor', 'user', 'version', 'episode_end_day',\n",
    "       'day_full', 'day_short', 'weekend_weekday', 'start_time_of_day',\n",
    "       'end_time_of_day', 'start_day', 'end_day', 'start_hour','end_hour','days_diff', 'duration',\n",
    "       'counter', 'week','episode_start_day',]]\n",
    "def rename_columns(df):\n",
    "    df[\"episode_start_day\"]=df[\"starttime\"].apply(datetime_date)\n",
    "    df[\"episode_end_day\"]=df[\"endtime\"].apply(datetime_date)\n",
    "    df[\"day_full\"]=df[\"starttime\"].apply(date_to_day_full)\n",
    "    df[\"day_short\"]=df[\"starttime\"].apply(date_to_day_short)\n",
    "    df[\"weekend_weekday\"]=df[\"day_full\"].apply(weekday_weeekend)\n",
    "    df[\"start_time_of_day\"]=df[\"starttime\"].map(time_to_time_of_day)\n",
    "    df[\"end_time_of_day\"]=df[\"endtime\"].map(time_to_time_of_day)\n",
    "    df['start_day'] = pd.to_datetime(df['episode_start_day'])\n",
    "    df['end_day'] = pd.to_datetime(df['episode_end_day'])\n",
    "    df['start_hour'] = df['starttime'].apply(round_to_hour).dt.hour\n",
    "    df['end_hour'] = df['endtime'].apply(round_to_hour).dt.hour\n",
    "    df[\"days_diff\"]=(df['episode_end_day'] - df['episode_start_day']).dt.days\n",
    "    df=df.loc[df[\"days_diff\"] < 2]\n",
    "    # calculating the duration of episodes in minutes\n",
    "    df['duration'] = df['endtime'].sub(df['starttime'], axis=0)\n",
    "    df['duration'] = df['duration'] / np.timedelta64(1, 'm')\n",
    "    return df\n",
    "    \n",
    "def impute_df_users_episodes(df_users_episodes):\n",
    "    df_users_episodes['same_eps'] = 0\n",
    "    new_data = []\n",
    "    drop_rows = []\n",
    "    for idx, row in df_users_episodes.iterrows():\n",
    "        start_date = row['starttime'].date()\n",
    "        end_date = row['endtime'].date()\n",
    "        start_time = row['starttime']\n",
    "        different_date = 0\n",
    "        data = row.to_list()\n",
    "        while start_date != end_date:\n",
    "            different_date = 1\n",
    "            end_time = start_time.replace(hour = 23, minute = 59, second = 59)\n",
    "            data[1] = start_time\n",
    "            data[2] = end_time\n",
    "            data[-2] = start_date\n",
    "            data[-1] = 1\n",
    "            insert_data = data.copy()\n",
    "            new_data.append(insert_data)\n",
    "            start_date = start_date + timedelta(days = 1)\n",
    "            start_time = datetime(start_date.year, start_date.month, start_date.day)\n",
    "        if different_date == 1:\n",
    "            data[1] = start_time\n",
    "            data[2] = row['endtime']\n",
    "            data[-2] = start_date\n",
    "            data[-1] = 1\n",
    "            new_data.append(data)\n",
    "            drop_rows.append(idx)\n",
    "    if drop_rows:\n",
    "        df_users_episodes.drop(index = drop_rows, inplace = True)\n",
    "    cols = df_users_episodes.columns.to_list()\n",
    "    df_impute = pd.DataFrame(new_data, columns = cols)\n",
    "    df_users_episodes = pd.concat([df_users_episodes, df_impute], ignore_index = True)\n",
    "    df_users_episodes.sort_values(by = 'starttime', inplace = True )\n",
    "    df_users_episodes.reset_index(drop = True, inplace = True)\n",
    "    return df_users_episodes\n",
    "\n",
    "def split_hours(df):\n",
    "    new_data = []\n",
    "    drop_rows = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        start_time = row['starttime']\n",
    "        start_hour = start_time.hour\n",
    "        end_hour = row['endtime'].hour\n",
    "        \n",
    "        different_hour = 0\n",
    "        data = row.to_list()\n",
    "        while start_hour != end_hour:\n",
    "            different_hour = 1\n",
    "            \n",
    "            end_time = start_time.replace(hour = start_hour, minute = 59, second = 59)\n",
    "            data[1] = start_time\n",
    "            data[2] = end_time\n",
    "\n",
    "            new_data.append(data.copy())\n",
    "\n",
    "            start_hour = start_hour + 1\n",
    "            if start_hour == 24:\n",
    "                start_hour=0\n",
    "            start_time = start_time.replace(hour = start_hour, minute = 0, second = 0)\n",
    "\n",
    "        if different_hour == 1:\n",
    "            data[1] = start_time\n",
    "            data[2] = row['endtime']            \n",
    "            new_data.append(data)    \n",
    "            drop_rows.append(idx)\n",
    "#     print(drop_rows)\n",
    "    if drop_rows:\n",
    "        df.drop(index = drop_rows, inplace = True)\n",
    "        \n",
    "    cols = df.columns.to_list()\n",
    "    df_hourly = pd.DataFrame(new_data, columns = cols)\n",
    "    df_hourly = pd.concat([df, df_hourly], ignore_index = True)\n",
    "    df_hourly.sort_values(by = 'starttime', inplace = True )\n",
    "    df_hourly.reset_index(drop = True, inplace = True)\n",
    "    return df_hourly\n",
    "print(len(pre_final_df_splitted))\n",
    "final_df_splitted = impute_df_users_episodes(pre_final_df_splitted)\n",
    "print(len(final_df_splitted))\n",
    "\n",
    "pre_final_df_splitted=final_df_splitted.copy()\n",
    "final_df_splitted_hour = split_hours(pre_final_df_splitted)\n",
    "print(len(final_df_splitted))\n",
    "print(len(final_df_splitted_hour))\n",
    "# print(fina)\n",
    "# final_df_splitted.rename(columns={'user_context':'Location','tag_name':\"Stressor\"}, inplace=True)\n",
    "# final_df_splitted_hour.rename(columns={'user_context':'Location','tag_name':\"Stressor\"}, inplace=True)\n",
    "final_df_splitted=rename_columns(final_df_splitted)\n",
    "# print(final_df_splitted.loc[final_df_splitted[\"user\"]==\"87d7d705-1cdf-36ae-9554-81afff640543\"]).tail(5)\n",
    "final_df_splitted_hour=rename_columns(final_df_splitted_hour)\n",
    "final_df_splitted_not_deleted=final_df_splitted.loc[final_df_splitted.deleted==0]\n",
    "final_df_splitted_not_deleted_hour=final_df_splitted_hour.loc[final_df_splitted_hour.deleted==0]\n",
    "    # this filters selected and user generated episodes\n",
    "final_df_splitted_selected=final_df_splitted_not_deleted.loc[(final_df_splitted_not_deleted.selected==1) & ( final_df_splitted_not_deleted.user_generated != 1)]\n",
    "final_df_splitted_stressed=final_df_splitted_selected.loc[final_df_splitted_selected[\"user_rating\"].isin([\"Unsure\", \"Probably stressed\",\"Stressed\"])]\n",
    "final_df_splitted_not_stressed=final_df_splitted_selected.loc[final_df_splitted_selected[\"user_rating\"].isin([ 'Probably not stressed', 'Not stressed'])]\n",
    "\n",
    "#     final_df_selected=final_df_not_deleted.loc[(final_df_not_deleted.selected==1)]\n",
    "final_df_splitted_selected_hour=final_df_splitted_not_deleted_hour.loc[(final_df_splitted_not_deleted_hour.selected==1) & ( final_df_splitted_not_deleted_hour.user_generated != 1)]\n",
    "\n",
    "final_df_splitted_stressed_hour=final_df_splitted_selected_hour.loc[final_df_splitted_selected_hour[\"user_rating\"].isin([\"Unsure\", \"Probably stressed\",\"Stressed\"])]\n",
    "final_df_splitted_hour_not_stressed=final_df_splitted_selected_hour.loc[final_df_splitted_selected_hour[\"user_rating\"].isin([ 'Probably not stressed', 'Probably stressed'])]\n",
    "\n",
    "\n",
    "# final_df_splitted_selected.rename(columns={'user_context':'Location','tag_name':\"Stressor\"}, inplace=True)\n",
    "with open('/home/jupyter/sneupane/MOODS//pickled_files/final_df_splitted.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_df_splitted, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('/home/jupyter/sneupane/MOODS//pickled_files/pre_stressed_df_splitted.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_df_splitted_stressed, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('/home/jupyter/sneupane/MOODS//pickled_files/not_stressed_splitted_df.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_df_splitted_not_stressed, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('/home/jupyter/sneupane/MOODS//pickled_files/final_df_splitted_hour.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_df_splitted_hour, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('/home/jupyter/sneupane/MOODS//pickled_files/pre_stressed_df_splitted_hour.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_df_splitted_stressed_hour, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('/home/jupyter/sneupane/MOODS//pickled_files/not_stressed_splitted_df_hour.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_df_splitted_hour_not_stressed, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_final_df_splitted.start_hour.unique()\n",
    "# final_df_splitted_selected.loc[final_df_splitted_selected[\"user\"]==\"87d7d705-1cdf-36ae-9554-81afff640543\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_hours(df):\n",
    "#     new_data = []\n",
    "#     drop_rows = []\n",
    "\n",
    "#     for idx, row in df.iterrows():\n",
    "#         start_time = row['starttime']\n",
    "#         start_hour = start_time.hour\n",
    "#         end_hour = row['endtime'].hour\n",
    "        \n",
    "#         different_hour = 0\n",
    "#         data = row.to_list()\n",
    "#         while start_hour != end_hour:\n",
    "#             different_hour = 1\n",
    "\n",
    "#             end_time = start_time.replace(hour = start_hour, minute = 59, second = 59)\n",
    "#             data[1] = start_time\n",
    "#             data[2] = end_time\n",
    "\n",
    "#             new_data.append(data.copy())\n",
    "\n",
    "#             start_hour = start_hour + 1\n",
    "#             start_time = start_time.replace(hour = start_hour, minute = 0, second = 0)\n",
    "\n",
    "#         if different_hour == 1:\n",
    "#             data[1] = start_time\n",
    "#             data[2] = row['endtime']            \n",
    "#             new_data.append(data)    \n",
    "#             drop_rows.append(idx)\n",
    "# #     print(drop_rows)\n",
    "#     if drop_rows:\n",
    "#         df.drop(index = drop_rows, inplace = True)\n",
    "        \n",
    "#     cols = df.columns.to_list()\n",
    "#     df_hourly = pd.DataFrame(new_data, columns = cols)\n",
    "#     df_hourly = pd.concat([df, df_hourly], ignore_index = True)\n",
    "#     df_hourly.sort_values(by = 'starttime', inplace = True )\n",
    "#     df_hourly.reset_index(drop = True, inplace = True)\n",
    "#     return df_hourly\n",
    "\n",
    "# final_df_splitted_hour = split_hours(mod_final_df_splitted)\n",
    "# print(len(mod_final_df_splitted),len(final_df_splitted_hour))\n",
    "# final_df_splitted_hour.rename(columns={'user_context':'Location','tag_name':\"Stressor\"}, inplace=True)\n",
    "# mod_final_df_splitted_hour=add_custom_cols(final_df_splitted_hour)\n",
    "# mod_final_df_splitted_hour_not_deleted=mod_final_df_splitted_hour.loc[mod_final_df_splitted_hour.deleted==0]\n",
    "#     # this filters selected and user generated episodes\n",
    "# final_df_splitted_hour_selected=mod_final_df_splitted_hour_not_deleted.loc[(mod_final_df_splitted_hour_not_deleted.selected==1) & ( mod_final_df_splitted_hour_not_deleted.user_generated != 1)]\n",
    "# #     final_df_selected=final_df_not_deleted.loc[(final_df_not_deleted.selected==1)]\n",
    "# final_df_selected_splitted_hour_not_stressed=final_df_splitted_hour_selected.loc[final_df_splitted_hour_selected[\"user_rating\"].isin([ 'Probably not stressed', 'Probably stressed'])]\n",
    "# final_df_splitted_hour_selected=final_df_splitted_hour_selected.loc[final_df_splitted_hour_selected[\"user_rating\"].isin([\"Unsure\", \"Probably stressed\",\"Stressed\"])]\n",
    "\n",
    "# final_df_splitted_selected.rename(columns={'user_context':'Location','tag_name':\"Stressor\"}, inplace=True)\n",
    "# with open('/home/jupyter/sneupane/MOODS//pickled_files/final_df_splitted_hour.pickle', 'wb') as handle:\n",
    "#     pickle.dump(mod_final_df_splitted_hour, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('/home/jupyter/sneupane/MOODS//pickled_files/pre_episodes_df_splitted_hour.pickle', 'wb') as handle:\n",
    "#     pickle.dump(final_df_splitted_hour_selected, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('/home/jupyter/sneupane/MOODS//pickled_files/not_stressed_splitted_df_hour.pickle', 'wb') as handle:\n",
    "#     pickle.dump(final_df_selected_splitted_hour_not_stressed, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_pickled_files():\n",
    "    \n",
    "    with open('/home/jupyter/sneupane/MOODS//pickled_files/pre_stressed_df.pickle', 'rb') as handle:\n",
    "        pre_final_df_stressed = pickle.load(handle)\n",
    "    with open('/home/jupyter/sneupane/MOODS//pickled_files/pre_stressed_df_splitted.pickle', 'rb') as handle:\n",
    "        pre_final_df_stressed_splitted = pickle.load(handle)\n",
    "    with open('/home/jupyter/sneupane/MOODS//pickled_files/pre_stressed_df_splitted_hour.pickle', 'rb') as handle:\n",
    "        pre_final_df_stressed_splitted_hour = pickle.load(handle)\n",
    "    with open('/home/jupyter/sneupane/MOODS//pickled_files/pre_self_df.pickle', 'rb') as handle:\n",
    "        pre_self_df = pickle.load(handle)\n",
    "    with open('/home/jupyter/sneupane/MOODS//pickled_files/user_profile.pickle', 'rb') as handle:\n",
    "        user_profile = pickle.load(handle)\n",
    "        \n",
    "    return pre_final_df_stressed,pre_final_df_stressed_splitted, pre_final_df_stressed_splitted_hour,pre_self_df,user_profile\n",
    "\n",
    "\n",
    "\n",
    "pre_final_df_stressed,pre_final_df_stressed_splitted, pre_final_df_stressed_splitted_hour,pre_self_df,user_profile=read_pickled_files()\n",
    "\n",
    "def correction_stressor_location(pre_final_df_selected,user_to_correct,datatype,orig,corr,date_time):\n",
    "    appended_data =[]\n",
    "    for user in user_profile:\n",
    "        temp_df=pre_final_df_selected.loc[pre_final_df_selected[\"user\"]==user]\n",
    "        if user==user_to_correct:          \n",
    "#             print(orig,corr)\n",
    "            \n",
    "            temp_df[datatype].replace(orig,corr,inplace=True)\n",
    "#             print(temp_df[datatype])\n",
    "#             p = Path(os.getcwd())\n",
    "\n",
    "#             path = str(Path(p.parent))\n",
    "            conn=sqlite3.connect(\"/home/jupyter/sneupane/MOODS/database/moods_loc_stress.db\")               \n",
    "            c=conn.cursor() \n",
    "            c.execute(\"INSERT INTO loc_stress_corr VALUES (?,?,?,?,?)\",(user_to_correct,datatype,orig,corr,date_time))\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "        appended_data.append(temp_df)\n",
    "    final_df = pd.concat(appended_data)\n",
    "    return final_df\n",
    "\n",
    "def correction_stressor_location_csv(pre_final_df_selected,correction_df):\n",
    "    appended_data =[]\n",
    "    for user in user_profile:\n",
    "\n",
    "        temp_df=pre_final_df_selected.loc[pre_final_df_selected[\"user\"]==user]\n",
    "        corr_df=correction_df.loc[correction_df[\"user\"]==user]\n",
    "        corr_df[\"original\"]=corr_df[\"original\"].str.lower()\n",
    "        corr_df[\"correction\"]=corr_df[\"correction\"].str.lower()\n",
    "        if len(corr_df) > 0:\n",
    "\n",
    "            location_corr=corr_df.loc[corr_df[\"datatype\"]==\"Location\"]\n",
    "            if len (location_corr) > 0:\n",
    "                corrections_loc = dict(zip(location_corr.original, location_corr.correction))\n",
    "                temp_df[\"Location\"].replace(corrections_loc,inplace=True)\n",
    "                \n",
    "            stressor_corr=corr_df.loc[corr_df[\"datatype\"]==\"Stressor\"]\n",
    "            if len(stressor_corr) > 0:             \n",
    "            \n",
    "                corrections_str = dict(zip(stressor_corr.original, stressor_corr.correction))\n",
    "                \n",
    "                temp_df[\"Stressor\"].replace(corrections_str,inplace=True)\n",
    "\n",
    "#                 print(temp_df.Stressor.unique())\n",
    "            p = Path(os.getcwd())\n",
    "\n",
    "            path = str(Path(p.parent))\n",
    "            conn=sqlite3.connect(\"/home/jupyter/sneupane/MOODS/database/moods_loc_stress.db\")               \n",
    "            c=conn.cursor()\n",
    "            for index, row in corr_df.iterrows():\n",
    "                c.execute(\"INSERT INTO loc_stress_corr VALUES (?,?,?,?,?)\",(user,row[\"datatype\"],row[\"original\"],row[\"correction\"],row[\"date\"]))\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "        appended_data.append(temp_df)\n",
    "    final_df = pd.concat(appended_data)\n",
    "    return final_df\n",
    "\n",
    "def correction_stressor_location_nodatabase(pre_final_df_selected,user_to_correct,datatype,orig,corr,date_time):\n",
    "    appended_data =[]\n",
    "    for user in user_profile:\n",
    "        temp_df=pre_final_df_selected.loc[pre_final_df_selected[\"user\"]==user]\n",
    "        if user==user_to_correct:          \n",
    "#             print(orig,corr)\n",
    "            \n",
    "            temp_df[datatype].replace(orig,corr,inplace=True)\n",
    "#             print(temp_df[datatype])\n",
    "        appended_data.append(temp_df)\n",
    "    final_df = pd.concat(appended_data)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "#run this if  change is made\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this if  change is made\n",
    "\n",
    "correction_df=pd.read_csv(\"/home/jupyter/sneupane/MOODS//csv_files/Corrections.csv\")\n",
    "# for index, row in correction_df.iterrows():\n",
    "#     final_df=correction_stressor_location(pre_final_df_selected,row[\"user\"],row[\"datatype\"],row[\"original\"],row[\"correction\"],row[\"date\"])\n",
    "#     final_df_splitted=correction_stressor_location(pre_final_df_selected_splitted,row[\"user\"],row[\"datatype\"],row[\"original\"],row[\"correction\"],row[\"date\"])\n",
    "#     final_df_splitted_hour=correction_stressor_location(pre_final_df_selected_splitted_hour,row[\"user\"],row[\"datatype\"],row[\"original\"],row[\"correction\"],row[\"date\"])\n",
    "#     if row[\"done\"]==0:        \n",
    "#         correction_df.at[index,'done'] = 1\n",
    "# correction_df.to_csv(\"/home/jupyter/sneupane/MOODS//csv_files/Correction.csv\",index=False)\n",
    "\n",
    "# stressed_df=correction_stressor_location(pre_final_df_stressed,\"136112f5-b332-34f7-89e9-4a0e6c5f54bd\",\"Location\",\"home \",\"home\",\"2021-05-21\")\n",
    "# stressed_df_splitted=correction_stressor_location(pre_final_df_stressed_splitted,\"136112f5-b332-34f7-89e9-4a0e6c5f54bd\",\"Location\",\"home \",\"home\",\"2021-05-21\")\n",
    "# stressed_df_splitted_hour=correction_stressor_location(pre_final_df_stressed_splitted_hour,\"136112f5-b332-34f7-89e9-4a0e6c5f54bd\",\"Location\",\"home \",\"home\",\"2021-05-21\")\n",
    "\n",
    "# stressed_df=correction_stressor_location(stressed_df,\"e792585a-48a2-36ae-9a50-de0605f76829\",\"Location\",\"Home\",\"home\",\"2021-09-02\")\n",
    "# stressed_df_splitted=correction_stressor_location(stressed_df_splitted,\"e792585a-48a2-36ae-9a50-de0605f76829\",\"Location\",\"Home\",\"home\",\"2021-09-02\")\n",
    "# stressed_df_splitted_hour=correction_stressor_location(stressed_df_splitted_hour,\"e792585a-48a2-36ae-9a50-de0605f76829\",\"Location\",\"Home\",\"home\",\"2021-09-02\")\n",
    "stressed_df=correction_stressor_location_csv(pre_final_df_stressed,correction_df)\n",
    "self_df=correction_stressor_location_csv(pre_self_df,correction_df)\n",
    "stressed_df_splitted=correction_stressor_location_csv(pre_final_df_stressed_splitted,correction_df)\n",
    "stressed_df_splitted_hour=correction_stressor_location_csv(pre_final_df_stressed_splitted_hour,correction_df)\n",
    "# final_df=correction_stressor_location(pre_final_df_selected,\"136112f5-b332-34f7-89e9-4a0e6c5f54bd\",\"Location\",\"home \",\"home\",\"2021-05-21\")\n",
    "\n",
    "\n",
    "with open('/home/jupyter/sneupane/MOODS//pickled_files/stressed_df.pickle', 'wb') as handle:\n",
    "    pickle.dump(stressed_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('/home/jupyter/sneupane/MOODS//pickled_files/self_df.pickle', 'wb') as handle:\n",
    "    pickle.dump(self_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "if save_chi:    \n",
    "    with open('/home/jupyter/sneupane/MOODS/analysis/papers/CHI/dataframe/stressed_df.pickle', 'wb') as handle:\n",
    "        pickle.dump(stressed_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "with open('/home/jupyter/sneupane/MOODS//pickled_files/stressed_df_splitted.pickle', 'wb') as handle:\n",
    "    pickle.dump(stressed_df_splitted, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('/home/jupyter/sneupane/MOODS//pickled_files/stressed_df_splitted_hour.pickle', 'wb') as handle:\n",
    "    pickle.dump(stressed_df_splitted_hour, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df_splitted=correction_stressor_location(pre_final_df_selected_splitted,\"136112f5-b332-34f7-89e9-4a0e6c5f54bd\",\"Location\",\"home \",\"home\",\"2021-05-21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # final_df_splitted\n",
    "# with open('/home/jupyter/sneupane/MOODS/pickled_files/stressed_df_splitted_hour.pickle', 'rb') as handle:\n",
    "#     final_df_stressed = pickle.load(handle)\n",
    "# final_df_stressed['duration'] = final_df_stressed['endtime'].sub(final_df_stressed['starttime'], axis=0)\n",
    "# final_df_stressed['duration'] = final_df_stressed['duration'] / np.timedelta64(1, 'm')\n",
    "# test=final_df_stressed.loc[final_df_stressed[\"user\"]==\"af5f95f0-09cc-3a0a-b283-1a0ae724e43c\"]\n",
    "# test_stressed=test.loc[test[\"Stressor\"].notnull()]\n",
    "# test_stressed=test.loc[test[\"day_full\"].isin([\"Friday\",\"Thursday\"])] \n",
    "# test_stressed[[\"starttime\",\"endtime\",\"duration\",\"day_full\",\"Stressor\",\"value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickled_files():\n",
    "    \n",
    "    with open('/home/jupyter/sneupane/MOODS/pickled_files/stressed_df.pickle', 'rb') as handle:\n",
    "        final_df_stressed = pickle.load(handle)\n",
    "\n",
    "    with open('/home/jupyter/sneupane/MOODS/pickled_files/user_profile.pickle', 'rb') as handle:\n",
    "        user_profile = pickle.load(handle)\n",
    "        \n",
    "    return final_df_stressed, user_profile\n",
    "\n",
    "\n",
    "def get_list_from_users(final_df_selected,user_profile):   \n",
    "    final_df_selected=final_df_selected.loc[final_df_selected['Stressor'].notnull()]\n",
    "    final_df_selected.Location = final_df_selected.Location.fillna('')\n",
    "#     final_df_selected=final_df_selected.loc[final_df_selected['Location'].notnull()]\n",
    "    users_stressors={}\n",
    "    users_location={}\n",
    "    for user in user_profile:\n",
    "        temp_df=final_df_selected.loc[final_df_selected[\"user\"]==user]\n",
    "        stressor_list=temp_df.Stressor.unique().tolist()\n",
    "        stressor_list_mod=[i.strip() for i in stressor_list]\n",
    "        location_list=temp_df.Location.unique().tolist()\n",
    "        location_list_mod=[i.strip() for i in location_list]\n",
    "        users_stressors[user]=stressor_list_mod\n",
    "        users_location[user]=location_list_mod\n",
    "    return users_stressors,users_location\n",
    "\n",
    "def convert_to_df(input_list):\n",
    "    final_df=pd.DataFrame()\n",
    "    for user in input_list:\n",
    "        user_vis_week={}       \n",
    "        temp=input_list[user]\n",
    "#         print(corrections)\n",
    "        if len(temp)>0:\n",
    "            df = pd.DataFrame()\n",
    "            df[\"user\"]=[user]*len(temp)\n",
    "            df[\"User input\"]=temp\n",
    "            df=df.reset_index()\n",
    "            final_df = pd.concat([final_df,df])\n",
    "\n",
    "    return final_df\n",
    "\n",
    "final_df_stressed,user_profile=read_pickled_files()\n",
    "\n",
    "\n",
    "users_stressors,users_location=get_list_from_users(final_df_stressed,users_today)\n",
    "users_stressors_df=convert_to_df(users_stressors)\n",
    "users_locations_df=convert_to_df(users_location)\n",
    "users_stressors_df.to_csv(\"/home/jupyter/sneupane/MOODS/csv_files/users_stressors_df.csv\",index=False)\n",
    "users_locations_df.to_csv(\"/home/jupyter/sneupane/MOODS/csv_files/users_locations_df.csv\",index=False)\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/users_stressors.pickle', 'wb') as handle:\n",
    "        pickle.dump(users_stressors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/users_location.pickle', 'wb') as handle:\n",
    "        pickle.dump(users_location, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "def read_pickled_files():\n",
    "    \n",
    "    with open('/home/jupyter/sneupane/MOODS/pickled_files/users_stressors.pickle', 'rb') as handle:\n",
    "        users_stressors = pickle.load(handle)\n",
    "    with open('/home/jupyter/sneupane/MOODS/pickled_files/users_location.pickle', 'rb') as handle:\n",
    "        users_location = pickle.load(handle)        \n",
    "    return users_stressors, users_location\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def abr_create(word):\n",
    "    tokens=word.split()\n",
    "#     print(tokens)\n",
    "    string=\"\"\n",
    "    if len(tokens) > 1:\n",
    "        for word in tokens:\n",
    "    #         if word != \"and\":\n",
    "                string += str(word[0])\n",
    "        if len(string) == 1:\n",
    "            return []\n",
    "        else:\n",
    "            string=re.sub(\"([^a-zA-Z0-9])\", \"\", string)\n",
    "            return string.upper()\n",
    "    else:\n",
    "        return word.replace(\" \", \"\")\n",
    "    \n",
    "    \n",
    "def get_abbreviated(list_of_words):\n",
    "    abbr_list={}\n",
    "    for user in list_of_words:\n",
    "        created_abr=[]\n",
    "        temp={}\n",
    "        if len(list_of_words[user])>0:\n",
    "            for i in list_of_words[user]:\n",
    "                if i != None:\n",
    "                    count=0\n",
    "                    abbr=abr_create(i)                    \n",
    "                    if abbr in created_abr:                        \n",
    "                        count+=1\n",
    "                        temp[i]=abbr+str(count)\n",
    "                    else:\n",
    "                        temp[i]=abbr\n",
    "                    created_abr.append(abbr)\n",
    "        temp[\"Others\"]=\"Others\"\n",
    "        abbr_list[user]=temp\n",
    "    return abbr_list\n",
    "\n",
    "\n",
    "\n",
    "users_stressors,users_location=read_pickled_files()\n",
    "stressor_abbr=get_abbreviated(users_stressors)\n",
    "location_abbr=get_abbreviated(users_location)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/stressor_abbr.pickle', 'wb') as handle:\n",
    "        pickle.dump(stressor_abbr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/location_abbr.pickle', 'wb') as handle:\n",
    "        pickle.dump(location_abbr, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users_location\n",
    "# location_abbr[\"5029590e-e207-3777-97e7-14041e7da291\"]\n",
    "\n",
    "\n",
    "# users_location[\"db66c264-3d1e-3b86-870e-ecc5744540e9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/user_profile.pickle', 'rb') as handle:\n",
    "    user_profile = pickle.load(handle)\n",
    "\n",
    "\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/user_days_week.pickle', 'rb') as handle:\n",
    "    user_days_week = pickle.load(handle)\n",
    "\n",
    "    \n",
    "\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/user_days_week.pickle', 'rb') as handle:\n",
    "    user_days_week = pickle.load(handle)\n",
    "\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/final_df.pickle', 'rb') as handle:\n",
    "    final_df = pickle.load(handle)\n",
    "    \n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/stressed_df.pickle', 'rb') as handle:\n",
    "    final_df_selected = pickle.load(handle)\n",
    "    \n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/final_df_splitted.pickle', 'rb') as handle:\n",
    "    final_df_splitted = pickle.load(handle)\n",
    "    \n",
    "\n",
    "final_df_splitted=final_df_splitted.loc[final_df_splitted[\"user_generated\"]==0]\n",
    "final_df_splitted=final_df_splitted.loc[  ( final_df_splitted.episode_class != -1)]\n",
    "final_df_splitted=final_df_splitted.loc[final_df_splitted[\"episode_class\"].isin([0.0,2.0])]\n",
    "grp_eps_data = final_df_splitted.groupby(['user', 'episode_start_day'])\n",
    "day_hour_minutes = {}\n",
    "try:    \n",
    "    for grp_key, group in grp_eps_data:\n",
    "            if grp_key[0] not in day_hour_minutes:\n",
    "                day_hour_minutes[grp_key[0]] = {}\n",
    "            if grp_key[1] not in day_hour_minutes[grp_key[0]]:\n",
    "                day_hour_minutes[grp_key[0]][grp_key[1]] = {st:0 for st in range(0, 24)}\n",
    "                \n",
    "            for idx, row in group.iterrows():\n",
    "                start_hr = row['starttime'].hour\n",
    "                end_hr = row['endtime'].hour\n",
    "                if start_hr == end_hr:\n",
    "                    dur = (row['endtime'] - row['starttime']).total_seconds() / 60\n",
    "                    day_hour_minutes[grp_key[0]][grp_key[1]][start_hr] += dur\n",
    "                    \n",
    "                elif start_hr != end_hr:\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    start_hr_dur = ((row['starttime'].replace(hour = start_hr + 1, minute = 0, second = 0, microsecond = 0)\n",
    "                                        - row['starttime']).total_seconds() / 60)\n",
    "                    day_hour_minutes[grp_key[0]][grp_key[1]][start_hr] += start_hr_dur\n",
    "                    for hr in range(start_hr + 1, end_hr):\n",
    "                        day_hour_minutes[grp_key[0]][grp_key[1]][hr] += 60\n",
    "                        \n",
    "                    end_hr_dur = ((row['endtime']-\n",
    "                                   row['endtime'].replace(hour = end_hr, minute = 0, second = 0, microsecond = 0))\n",
    "                                  .total_seconds() / 60)\n",
    "                    \n",
    "                    day_hour_minutes[grp_key[0]][grp_key[1]][end_hr] += end_hr_dur\n",
    "\n",
    "\n",
    "except:\n",
    "    print(\"No episode data exists for that participat in that day!\")\n",
    "\n",
    "# this returns a dictionary which contains user, date, hour and ppg duration\n",
    "final_df_splitted=final_df_splitted.loc[final_df_splitted[\"user_generated\"]==0]\n",
    "final_df_splitted=final_df_splitted.loc[  ( final_df_splitted.episode_class != -1)]\n",
    "final_df_splitted=final_df_splitted.loc[final_df_splitted[\"episode_class\"].isin([0.0,2.0])]\n",
    "grp_eps_data = final_df_splitted.groupby(['user', 'day_full'])\n",
    "day_week_hour_minutes = {}\n",
    "try:    \n",
    "    for grp_key, group in grp_eps_data:\n",
    "            if grp_key[0] not in day_week_hour_minutes:\n",
    "                day_week_hour_minutes[grp_key[0]] = {}\n",
    "            if grp_key[1] not in day_week_hour_minutes[grp_key[0]]:\n",
    "                day_week_hour_minutes[grp_key[0]][grp_key[1]] = {st:0 for st in range(0, 24)}\n",
    "                \n",
    "            for idx, row in group.iterrows():\n",
    "                start_hr = row['starttime'].hour\n",
    "                end_hr = row['endtime'].hour\n",
    "                if start_hr == end_hr:\n",
    "                    dur = (row['endtime'] - row['starttime']).total_seconds() / 60\n",
    "                    day_week_hour_minutes[grp_key[0]][grp_key[1]][start_hr] += dur\n",
    "                    \n",
    "                elif start_hr != end_hr:\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    start_hr_dur = ((row['starttime'].replace(hour = start_hr + 1, minute = 0, second = 0, microsecond = 0)\n",
    "                                        - row['starttime']).total_seconds() / 60)\n",
    "                    day_week_hour_minutes[grp_key[0]][grp_key[1]][start_hr] += start_hr_dur\n",
    "                    for hr in range(start_hr + 1, end_hr):\n",
    "                        day_week_hour_minutes[grp_key[0]][grp_key[1]][hr] += 60\n",
    "                        \n",
    "                    end_hr_dur = ((row['endtime']-\n",
    "                                   row['endtime'].replace(hour = end_hr, minute = 0, second = 0, microsecond = 0))\n",
    "                                  .total_seconds() / 60)\n",
    "                    \n",
    "                    day_week_hour_minutes[grp_key[0]][grp_key[1]][end_hr] += end_hr_dur\n",
    "\n",
    "\n",
    "except:\n",
    "    print(\"No episode data exists for that participant in that day!\")\n",
    "\n",
    "    \n",
    "week_day_hour_minutes={}\n",
    "for user in users_today:\n",
    "#     print(user)\n",
    "    user_day_hour_minutes=\"\"\n",
    "    user_week_date_map=user_days_week[user][0][\"Weeks_days\"]\n",
    "    if user in day_hour_minutes:\n",
    "        user_day_hour_minutes=day_hour_minutes[user]\n",
    "\n",
    "    if len(user_day_hour_minutes)>0:\n",
    "        print(user)\n",
    "        \n",
    "        dates=list(user_day_hour_minutes.keys())\n",
    "        dates_map=[]\n",
    "        for date in dates:\n",
    "            if date >= user_profile[user][2]:\n",
    "                dates_map.append(user_week_date_map[date])\n",
    "    \n",
    "        week_map={}\n",
    "        for date in dates:\n",
    "            if date >= user_profile[user][2]:\n",
    "                week_map[date]=user_week_date_map[date]      \n",
    "\n",
    "        mod_week_map={}\n",
    "        for key, val in week_map.items():\n",
    "            mod_week_map[val] = mod_week_map.get(val, []) + [key]\n",
    "        temp_week={}\n",
    "        for week in mod_week_map:\n",
    "            day_hour_duration={}\n",
    "            dates=mod_week_map[week]\n",
    "            for date in dates:                \n",
    "                \n",
    "                day_hour_duration[date]=user_day_hour_minutes[date]\n",
    "            temp_week[week]=day_hour_duration\n",
    "        week_day_hour_minutes[user]=temp_week\n",
    "        \n",
    "user_hour_mean_minutes={}\n",
    "for user in users_today:\n",
    "    hour_list_dict=dict()\n",
    "    hour_list_dict_mean={}\n",
    "    user_week_day_hour_minutes=\"\"\n",
    "    if user in week_day_hour_minutes:\n",
    "        user_week_day_hour_minutes=week_day_hour_minutes[user]\n",
    "    \n",
    "        for hour in range(0,24):\n",
    "            temp=[]\n",
    "            for week in user_week_day_hour_minutes:\n",
    "                for day in user_week_day_hour_minutes[week]:\n",
    "        #             print(users_day_hour_duration_user[week][day][hour])\n",
    "                    if user_week_day_hour_minutes[week][day][hour] > 0:\n",
    "                        temp.append(user_week_day_hour_minutes[week][day][hour])\n",
    "            hour_list_dict[hour]=temp\n",
    "\n",
    "\n",
    "        for hour in range(0,24):\n",
    "            if len(hour_list_dict[hour] ) > 0:\n",
    "                hour_list_dict_mean[hour]= statistics.mean(hour_list_dict[hour])\n",
    "            else:\n",
    "                hour_list_dict_mean[hour]=0\n",
    "        user_hour_mean_minutes[user]=hour_list_dict_mean\n",
    "    \n",
    "\n",
    "import pickle\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/day_hour_minutes.pickle', 'wb') as handle:\n",
    "    pickle.dump(day_hour_minutes, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/week_day_hour_minutes.pickle', 'wb') as handle:\n",
    "    pickle.dump(week_day_hour_minutes, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/user_hour_mean_minutes.pickle', 'wb') as handle:\n",
    "    pickle.dump(user_hour_mean_minutes, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/day_week_hour_minutes.pickle', 'wb') as handle:\n",
    "    pickle.dump(day_week_hour_minutes, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_day_hour_minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/stressed_df_splitted_hour.pickle', 'rb') as handle:\n",
    "    final_df_stressed = pickle.load(handle)\n",
    "#     final_df_stressed=final_df_stressed.loc[final_df_stressed[\"Location\"].notnull()]\n",
    "    final_df_stressed.Location.fillna(value=\"Unknown\", inplace=True)\n",
    "    final_df_stressed=final_df_stressed.loc[final_df_stressed[\"Stressor\"].notnull()]\n",
    "    final_df_stressed=final_df_stressed.loc[ ( final_df_stressed.user_generated != 1)& ( final_df_stressed.episode_class != -1)]\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/user_profile.pickle', 'rb') as handle:\n",
    "    user_profile = pickle.load(handle)\n",
    "\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/day_week_hour_minutes.pickle', 'rb') as handle:\n",
    "    day_week_hour_minutes = pickle.load(handle)\n",
    "\n",
    "    \n",
    "    \n",
    "def round_to_hour(dt):\n",
    "    dt_start_of_hour = dt.replace(minute=0, second=0, microsecond=0)\n",
    "    return dt_start_of_hour\n",
    "\n",
    "def hour_to_date(dt):\n",
    "    return dt.date()\n",
    "\n",
    "day_dict={\"Monday\":\"Mon\",\"Tuesday\":\"Tue\",\"Wednesday\":\"Wed\",\"Thursday\":\"Thu\",\"Friday\":\"Fri\",\"Saturday\":\"Sat\",\"Sunday\":\"Sun\"}\n",
    "\n",
    "#this function maps the date to day of the week\n",
    "\n",
    "def date_to_day_short(date1):\n",
    "#     print(date1)\n",
    "    return day_dict[calendar.day_name[date1.weekday()]]\n",
    "\n",
    "def episodes_to_hourly_duration(user,df):\n",
    "    df=df[df.user==user]\n",
    "    df=df.loc[df.Stressor.notnull()]\n",
    "    test_df=pd.DataFrame()\n",
    "    if len(df) > 0:\n",
    "#         print(user)\n",
    "        test=df[[\"starttime\",\"endtime\",\"episode_start_day\",\"episode_end_day\"]]\n",
    "        test[\"start_hour\"]=test[\"starttime\"].apply(round_to_hour)\n",
    "\n",
    "        test[\"end_hour\"]=test[\"endtime\"].apply(round_to_hour)\n",
    "        test1=test.query('start_hour==end_hour')\n",
    "        test2=test.query('start_hour!=end_hour')\n",
    "        test1[\"duration\"]=test1['endtime'].sub(test1['starttime'], axis=0)/np.timedelta64(1, 'm')\n",
    "        \n",
    "        test1=test1[[\"episode_start_day\",\"start_hour\",\"duration\"]]\n",
    "\n",
    "        test2[\"duration_start\"]=test2['starttime'].sub(test2['start_hour'], axis=0)/np.timedelta64(1, 'm')\n",
    "        test2[\"duration_start\"]=[60 - i for i in test2.duration_start.values ]\n",
    "        test2[\"duration_end\"]=test2['endtime'].sub(test2['end_hour'], axis=0)/np.timedelta64(1, 'm') \n",
    "#         print(test2.columns)\n",
    "\n",
    "        test_groupby_start_hour=test2.groupby([\"episode_start_day\",\"start_hour\"],as_index=False)[\"duration_start\"].sum()\n",
    "        test_groupby_end_hour=test2.groupby([\"episode_end_day\",\"end_hour\"],as_index=False)[\"duration_end\"].sum()\n",
    "\n",
    "        \n",
    "#         print(test_groupby_start_hour_stressor.head(5))\n",
    "        \n",
    "        columns=[\"date\",\"hour\",\"duration\"]\n",
    "        test1.columns=columns\n",
    "        test_groupby_start_hour.columns=columns\n",
    "        test_groupby_end_hour.columns=columns\n",
    "        test_df=test_groupby_start_hour.append(test_groupby_end_hour)\n",
    "        test_df=test_df.append(test1)\n",
    "\n",
    "        test_df=test_df.groupby([\"hour\"],as_index=False)[\"duration\"].sum()\n",
    "        test_df=test_df.sort_values([\"hour\"])\n",
    "\n",
    "\n",
    "\n",
    "        test_df[\"date\"]=test_df[\"hour\"].apply(hour_to_date)\n",
    "        test_df[\"user\"]=[user]*len(test_df)\n",
    "\n",
    "        return test_df\n",
    "    else:\n",
    "        return test_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cats = [ 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "def matrix_creation_duration(user,final_df_selected):\n",
    "    hourly_stressed_df=pd.DataFrame()\n",
    "#     stressor_df=pd.DataFrame()\n",
    "#     for user in users_today:\n",
    "    hourly_stressed_df=hourly_stressed_df.append(episodes_to_hourly_duration(user,final_df_selected))\n",
    "    if len(hourly_stressed_df)>0:\n",
    "    #         stressor_df=stressor_df.append(test2)\n",
    "\n",
    "        hourly_stressed_df[\"day\"]=hourly_stressed_df[\"date\"].apply(date_to_day_short)\n",
    "        hourly_stressed_df=hourly_stressed_df[[\"user\",\"date\",\"day\",\"hour\",\"duration\"]]\n",
    "        df=hourly_stressed_df[hourly_stressed_df.user==user]\n",
    "\n",
    "        df['day'] = pd.Categorical(df['day'], categories=cats, ordered=True)\n",
    "        df = df.sort_values(['day'])\n",
    "        df[\"hour_only\"]=df.hour.dt.hour\n",
    "        df=df[[\"date\",\"day\",\"hour_only\",\"duration\"]]\n",
    "        modified_df=pd.DataFrame()\n",
    "        for day in cats:\n",
    "\n",
    "            day_df=df.loc[df[\"day\"]==day]\n",
    "            day_df=day_df.groupby([\"hour_only\"],as_index=False)[\"duration\"].sum()\n",
    "        #     print(day_df.columns)\n",
    "            for i in range(0,24):        \n",
    "                day_df_hour=day_df.loc[day_df[\"hour_only\"]==i]\n",
    "        #         print(i)\n",
    "        #         print(day_df_hour.columns)\n",
    "                if len(day_df_hour) == 0:\n",
    "        #             new_row = pd.Series(data={\"hour_only\":i,\"duration\":0}, name='x')\n",
    "                    day_df.loc[len(day_df)]=[i,0]\n",
    "                day_df=day_df.sort_values(\"hour_only\")\n",
    "            day_df[\"day\"]=[day]*len(day_df)\n",
    "\n",
    "            modified_df=modified_df.append(day_df)\n",
    "    #         print(modified_df.head(5))\n",
    "    #                 print((day_df))\n",
    "        modified_df_pivot=modified_df.pivot(index=\"hour_only\",columns=\"day\",values=\"duration\")\n",
    "        final_list=[]\n",
    "    #         print(modified_df_pivot.head(5))\n",
    "        modified_df_pivot=modified_df_pivot[cats]\n",
    "        for index, row in modified_df_pivot.iterrows():\n",
    "            temp_list=row.tolist()\n",
    "            temp_list=[round(i,3) for i in temp_list]\n",
    "            final_list.append(temp_list)\n",
    "\n",
    "\n",
    "        return final_list\n",
    "    \n",
    "    \n",
    "    \n",
    "def episodes_to_hourly_duration_stressor(user,df):\n",
    "#      if user == \"d20e1bc7-de8d-38e4-9d97-09e64b88816d\":\n",
    "        df=df.loc[df.Stressor.notnull()]\n",
    "        df=df[df.user==user]\n",
    "        test_df=pd.DataFrame()\n",
    "        test_df1=pd.DataFrame()\n",
    "#         df=df.head(10)\n",
    "#         print(df.tag_name.values)\n",
    "   \n",
    "        if len(df) > 0:\n",
    "            \n",
    "            test=df[[\"starttime\",\"endtime\",\"episode_start_day\",\"episode_end_day\",\"Stressor\"]]\n",
    "            test[\"start_hour\"]=test[\"starttime\"].apply(round_to_hour)\n",
    "            test[\"end_hour\"]=test[\"endtime\"].apply(round_to_hour)\n",
    "            test1=test.query('start_hour==end_hour')\n",
    "            test2=test.query('start_hour!=end_hour')\n",
    "            test1[\"duration\"]=test1['endtime'].sub(test1['starttime'], axis=0)/np.timedelta64(1, 'm')\n",
    "            test1=test1[[\"episode_start_day\",\"start_hour\",\"duration\",\"Stressor\"]]\n",
    "            test1 = test1.groupby([\"episode_start_day\",\"start_hour\",\"Stressor\"],as_index=False)['duration'].sum()\n",
    "           \n",
    "#             print(x)\n",
    "#             test1=test1[idx]\n",
    "#             print(test1)\n",
    "            test2[\"duration_start\"]=test2['starttime'].sub(test2['start_hour'], axis=0)/np.timedelta64(1, 'm')\n",
    "            test2[\"duration_start\"]=[60 - i for i in test2.duration_start.values ]\n",
    "            test2[\"duration_end\"]=test2['endtime'].sub(test2['end_hour'], axis=0)/np.timedelta64(1, 'm')\n",
    "#             print(test2.head(1))\n",
    "            test_groupby_start_hour=test2.groupby([\"episode_start_day\",\"start_hour\",\"Stressor\"],as_index=False)[\"duration_start\"].sum()\n",
    "            test_groupby_end_hour=test2.groupby([\"episode_end_day\",\"end_hour\",\"Stressor\"],as_index=False)[\"duration_end\"].sum()\n",
    "            \n",
    "#             print(test_groupby_start_hour.head(5))\n",
    "            columns=[\"date\",\"hour\",\"Stressor\",\"duration\"]\n",
    "            test1.columns=columns\n",
    "            test_groupby_start_hour.columns=columns\n",
    "            test_groupby_end_hour.columns=columns\n",
    "            test_df=test_groupby_start_hour.append(test_groupby_end_hour)\n",
    "            test_df=test_df.append(test1)\n",
    "            test_df=test_df.groupby([\"hour\",\"Stressor\"],as_index=False)[\"duration\"].sum()\n",
    "            test_df=test_df.sort_values([\"hour\"])\n",
    "            test_df[\"date\"]=test_df[\"hour\"].apply(hour_to_date)\n",
    "            test_df[\"user\"]=[user]*len(test_df)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            return test_df\n",
    "        else:\n",
    "            return test_df\n",
    "\n",
    "# hourly_stressed_df=pd.DataFrame()\n",
    "\n",
    "\n",
    "cats = [ 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "def matrix_creation_stressor(user,final_df_selected):\n",
    "    \n",
    "    stressor_df=pd.DataFrame()\n",
    "\n",
    "\n",
    "    stressor_df=stressor_df.append(episodes_to_hourly_duration_stressor(user,final_df_selected))\n",
    "    if len(stressor_df)>0:\n",
    "        stressor_df[\"day\"]=stressor_df[\"date\"].apply(date_to_day_short)\n",
    "        stressor_df=stressor_df[[\"user\",\"date\",\"day\",\"hour\",\"Stressor\",\"duration\"]]\n",
    "        stressor_df=stressor_df.sort_values([\"user\",\"date\",\"hour\",\"duration\"],ascending=True)\n",
    "        stressor_df=stressor_df.drop_duplicates(subset=[\"user\",\"date\",\"hour\"], keep=\"last\")\n",
    "        df=stressor_df[stressor_df.user==user]   \n",
    "        df['day'] = pd.Categorical(df['day'], categories=cats, ordered=True)\n",
    "        df = df.sort_values(['day'])\n",
    "        df[\"hour_only\"]=df.hour.dt.hour\n",
    "        df=df[[\"day\",\"hour_only\",\"Stressor\",\"duration\"]]\n",
    "        modified_df=pd.DataFrame()\n",
    "        for day in cats:\n",
    "\n",
    "            day_df=df.loc[df[\"day\"]==day]\n",
    "            day_df=day_df.sort_values([\"day\",\"hour_only\",\"duration\"])\n",
    "    #         print(day_df)\n",
    "            day_df=day_df.drop_duplicates(subset=[\"day\",\"hour_only\"], keep=\"last\")\n",
    "\n",
    "            day_df=day_df[[\"hour_only\",\"Stressor\"]]\n",
    "    #         print(day_df)\n",
    "    #         day_df=day_df.groupby([\"hour_only\",\"\n",
    "    #                                \"],as_index=False)[\"duration\"].max()\n",
    "        #     print(day_df.columns)\n",
    "            for i in range(0,24):        \n",
    "                day_df_hour=day_df.loc[day_df[\"hour_only\"]==i]\n",
    "    #             print(i)\n",
    "    #             print(day_df_hour)\n",
    "                if len(day_df_hour) == 0:\n",
    "    #                 print(day,i)\n",
    "        #             new_row = pd.Series(data={\"hour_only\":i,\"duration\":0}, name='x')\n",
    "                    day_df=day_df.append({\"hour_only\":i,\"Stressor\":0},ignore_index=True)\n",
    "\n",
    "            day_df=day_df.sort_values(\"hour_only\")\n",
    "            day_df[\"day\"]=[day]*len(day_df)\n",
    "\n",
    "            modified_df=modified_df.append(day_df)\n",
    "    #         print(modified_df)\n",
    "    #                 print((day_df))\n",
    "        modified_df_pivot=modified_df.pivot(index=\"hour_only\",columns=\"day\",values=\"Stressor\")\n",
    "\n",
    "        final_list=[]\n",
    "    #         print(modified_df_pivot.head(5))\n",
    "        modified_df_pivot=modified_df_pivot[cats]\n",
    "    #     print(modified_df_pivot)\n",
    "        for index, row in modified_df_pivot.iterrows():\n",
    "            temp_list=row.tolist()\n",
    "    #         temp_list=[round(i,3) for i in temp_list]\n",
    "            final_list.append(temp_list)\n",
    "\n",
    "\n",
    "        return final_list\n",
    "    \n",
    "    \n",
    "def episodes_to_hourly_duration_likelihood(user,df):\n",
    "#      if user == \"d20e1bc7-de8d-38e4-9d97-09e64b88816d\":\n",
    "        df=df.loc[df.Stressor.notnull()]\n",
    "        df=df[df.user==user]\n",
    "        test_df=pd.DataFrame()\n",
    "        test_df1=pd.DataFrame()\n",
    "#         df=df.head(10)\n",
    "#         print(df.tag_name.values)\n",
    "   \n",
    "        if len(df) > 0:\n",
    "#             print(user)\n",
    "            test=df[[\"starttime\",\"endtime\",\"episode_start_day\",\"episode_end_day\",\"value\"]]\n",
    "            test[\"start_hour\"]=test[\"starttime\"].apply(round_to_hour)\n",
    "            test[\"end_hour\"]=test[\"endtime\"].apply(round_to_hour)\n",
    "            test1=test.query('start_hour==end_hour')\n",
    "            test2=test.query('start_hour!=end_hour')\n",
    "           \n",
    "            test1=test1[[\"episode_start_day\",\"start_hour\",\"value\"]]\n",
    "            test1 = test1.groupby([\"episode_start_day\",\"start_hour\"],as_index=False)['value'].mean()\n",
    "           \n",
    "\n",
    "#             print(test2.head(1))\n",
    "            test_groupby_start_hour=test2.groupby([\"episode_start_day\",\"start_hour\"],as_index=False)[\"value\"].mean()\n",
    "            test_groupby_end_hour=test2.groupby([\"episode_end_day\",\"end_hour\"],as_index=False)[\"value\"].mean()\n",
    "            \n",
    "#             print(test_groupby_start_hour.head(5))\n",
    "            columns=[\"date\",\"hour\",\"value\"]\n",
    "            test1.columns=columns\n",
    "            test_groupby_start_hour.columns=columns\n",
    "            test_groupby_end_hour.columns=columns\n",
    "            test_df=test_groupby_start_hour.append(test_groupby_end_hour)\n",
    "            test_df=test_df.append(test1)\n",
    "            test_df=test_df.groupby([\"hour\"],as_index=False)[\"value\"].mean()\n",
    "            test_df=test_df.sort_values([\"hour\"])\n",
    "            test_df[\"date\"]=test_df[\"hour\"].apply(hour_to_date)\n",
    "            test_df[\"user\"]=[user]*len(test_df)      \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            return test_df\n",
    "        else:\n",
    "            return test_df\n",
    "\n",
    "# hourly_stressed_df=pd.DataFrame()\n",
    "\n",
    "# value_df=value_df[[\"user\",\"date\",\"day\",\"hour\",\"tag_name\",\"duration\"]]\n",
    "# stressor_df=stressor_df.sort_values([\"user\",\"date\",\"hour\",\"duration\"],ascending=True)\n",
    "# stressor_df=stressor_df.drop_duplicates(subset=[\"user\",\"date\",\"hour\"], keep=\"last\")\n",
    "\n",
    "cats = [ 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "def matrix_creation_likelihood(user,final_df_selected):\n",
    "    value_df=pd.DataFrame()\n",
    "\n",
    "    \n",
    "    value_df=value_df.append(episodes_to_hourly_duration_likelihood(user,final_df_selected))\n",
    "    if len(value_df)>0:\n",
    "        value_df[\"day\"]=value_df[\"date\"].apply(date_to_day_short)\n",
    "        df=value_df[value_df.user==user]\n",
    "\n",
    "        df['day'] = pd.Categorical(df['day'], categories=cats, ordered=True)\n",
    "        df = df.sort_values(['day'])\n",
    "        df[\"hour_only\"]=df.hour.dt.hour\n",
    "        df=df[[\"day\",\"hour_only\",\"value\"]]\n",
    "        modified_df=pd.DataFrame()\n",
    "        for day in cats:\n",
    "\n",
    "            day_df=df.loc[df[\"day\"]==day]\n",
    "            day_df=day_df.groupby([\"hour_only\"],as_index=False)[\"value\"].mean()\n",
    "    # #         print(day_df)\n",
    "    #         day_df=day_df.drop_duplicates(subset=[\"day\",\"hour_only\"], keep=\"last\")\n",
    "\n",
    "    #         day_df=day_df[[\"hour_only\",\"tag_name\"]]\n",
    "    #         print(day_df)\n",
    "    #         day_df=day_df.groupby([\"hour_only\",\"\n",
    "    #                                \"],as_index=False)[\"duration\"].max()\n",
    "        #     print(day_df.columns)\n",
    "            for i in range(0,24):        \n",
    "                day_df_hour=day_df.loc[day_df[\"hour_only\"]==i]\n",
    "    #             print(i)\n",
    "    #             print(day_df_hour)\n",
    "                if len(day_df_hour) == 0:\n",
    "    #                 print(day,i)\n",
    "        #             new_row = pd.Series(data={\"hour_only\":i,\"duration\":0}, name='x')\n",
    "                    day_df=day_df.append({\"hour_only\":i,\"value\":0},ignore_index=True)\n",
    "\n",
    "            day_df=day_df.sort_values(\"hour_only\")\n",
    "            day_df[\"day\"]=[day]*len(day_df)\n",
    "\n",
    "            modified_df=modified_df.append(day_df)\n",
    "    #     print(modified_df.head(27))\n",
    "    #                 print((day_df))\n",
    "        modified_df_pivot=modified_df.pivot(index=\"hour_only\",columns=\"day\",values=\"value\")\n",
    "\n",
    "        final_list=[]\n",
    "    #         print(modified_df_pivot.head(5))\n",
    "        modified_df_pivot=modified_df_pivot[cats]\n",
    "    #     print(modified_df_pivot)\n",
    "        for index, row in modified_df_pivot.iterrows():\n",
    "            temp_list=row.tolist()\n",
    "    #         temp_list=[round(i,3) for i in temp_list]\n",
    "            final_list.append(temp_list)\n",
    "\n",
    "\n",
    "        return final_list\n",
    "    \n",
    "    \n",
    "def episodes_to_hourly_duration_location(user,df):\n",
    "#      if user == \"d20e1bc7-de8d-38e4-9d97-09e64b88816d\":\n",
    "    df=df.loc[df.Location.notnull()]\n",
    "    df=df[df.user==user]\n",
    "    test_df=pd.DataFrame()\n",
    "    test_df1=pd.DataFrame()\n",
    "#         df=df.head(10)\n",
    "#         print(df.tag_name.values)\n",
    "\n",
    "    if len(df) > 0:\n",
    "#         print(user)\n",
    "        test=df[[\"starttime\",\"endtime\",\"episode_start_day\",\"episode_end_day\",\"Location\"]]\n",
    "        test[\"start_hour\"]=test[\"starttime\"].apply(round_to_hour)\n",
    "        test[\"end_hour\"]=test[\"endtime\"].apply(round_to_hour)\n",
    "        test1=test.query('start_hour==end_hour')\n",
    "        test2=test.query('start_hour!=end_hour')\n",
    "        test1[\"duration\"]=test1['endtime'].sub(test1['starttime'], axis=0)/np.timedelta64(1, 'm')\n",
    "        test1=test1[[\"episode_start_day\",\"start_hour\",\"duration\",\"Location\"]]\n",
    "        test1 = test1.groupby([\"episode_start_day\",\"start_hour\",\"Location\"],as_index=False)['duration'].sum()\n",
    "\n",
    "#             print(x)\n",
    "#             test1=test1[idx]\n",
    "#             print(test1)\n",
    "        test2[\"duration_start\"]=test2['starttime'].sub(test2['start_hour'], axis=0)/np.timedelta64(1, 'm')\n",
    "        test2[\"duration_start\"]=[60 - i for i in test2.duration_start.values ]\n",
    "#         test2[\"duration_mid\"]=test1['end_hour'].sub(test1['start_hour'], axis=0)/np.timedelta64(1, 'm')\n",
    "        test2[\"duration_end\"]=test2['endtime'].sub(test2['end_hour'], axis=0)/np.timedelta64(1, 'm')\n",
    "#             print(test2.head(1))\n",
    "        test_groupby_start_hour=test2.groupby([\"episode_start_day\",\"start_hour\",\"Location\"],as_index=False)[\"duration_start\"].sum()\n",
    "        test_groupby_end_hour=test2.groupby([\"episode_end_day\",\"end_hour\",\"Location\"],as_index=False)[\"duration_end\"].sum()\n",
    "\n",
    "#             print(test_groupby_start_hour.head(5))\n",
    "        columns=[\"date\",\"hour\",\"Location\",\"duration\"]\n",
    "        test1.columns=columns\n",
    "        test_groupby_start_hour.columns=columns\n",
    "        test_groupby_end_hour.columns=columns\n",
    "        test_df=test_groupby_start_hour.append(test_groupby_end_hour)\n",
    "        test_df=test_df.append(test1)\n",
    "#         print(test_df)\n",
    "        test_df=test_df.groupby([\"hour\",\"Location\"],as_index=False)[\"duration\"].sum()\n",
    "        test_df=test_df.sort_values([\"hour\"])\n",
    "        test_df[\"date\"]=test_df[\"hour\"].apply(hour_to_date)\n",
    "        test_df[\"user\"]=[user]*len(test_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return test_df\n",
    "    else:\n",
    "        return test_df\n",
    "    \n",
    "# hourly_stressed_df=pd.DataFrame()\n",
    "\n",
    "\n",
    "cats = [ 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "def matrix_creation_location(user,final_df_selected):\n",
    "    \n",
    "    location_df=pd.DataFrame()\n",
    "    \n",
    "    location_df=location_df.append(episodes_to_hourly_duration_location(user,final_df_selected))\n",
    "    if len(location_df)>0:\n",
    "        location_df[\"day\"]=location_df[\"date\"].apply(date_to_day_short)\n",
    "        location_df=location_df[[\"user\",\"date\",\"day\",\"hour\",\"Location\",\"duration\"]]\n",
    "        location_df=location_df.sort_values([\"user\",\"date\",\"hour\",\"duration\"],ascending=True)\n",
    "        location_df=location_df.drop_duplicates(subset=[\"user\",\"date\",\"hour\"], keep=\"last\")\n",
    "        df=location_df[location_df.user==user]   \n",
    "        df['day'] = pd.Categorical(df['day'], categories=cats, ordered=True)\n",
    "        df = df.sort_values(['day'])\n",
    "        df[\"hour_only\"]=df.hour.dt.hour\n",
    "        df=df[[\"day\",\"hour_only\",\"Location\",\"duration\"]]\n",
    "        modified_df=pd.DataFrame()\n",
    "        for day in cats:\n",
    "\n",
    "            day_df=df.loc[df[\"day\"]==day]\n",
    "            day_df=day_df.sort_values([\"day\",\"hour_only\",\"duration\"])\n",
    "        #         print(day_df)\n",
    "            day_df=day_df.drop_duplicates(subset=[\"day\",\"hour_only\"], keep=\"last\")\n",
    "\n",
    "            day_df=day_df[[\"hour_only\",\"Location\"]]\n",
    "        #         print(day_df)\n",
    "        #         day_df=day_df.groupby([\"hour_only\",\"\n",
    "        #                                \"],as_index=False)[\"duration\"].max()\n",
    "        #     print(day_df.columns)\n",
    "            for i in range(0,24):        \n",
    "                day_df_hour=day_df.loc[day_df[\"hour_only\"]==i]\n",
    "        #             print(i)\n",
    "        #             print(day_df_hour)\n",
    "                if len(day_df_hour) == 0:\n",
    "        #                 print(day,i)\n",
    "        #             new_row = pd.Series(data={\"hour_only\":i,\"duration\":0}, name='x')\n",
    "                    day_df=day_df.append({\"hour_only\":i,\"Location\":0},ignore_index=True)\n",
    "\n",
    "            day_df=day_df.sort_values(\"hour_only\")\n",
    "            day_df[\"day\"]=[day]*len(day_df)\n",
    "\n",
    "            modified_df=modified_df.append(day_df)\n",
    "        #         print(modified_df)\n",
    "        #                 print((day_df))\n",
    "        modified_df_pivot=modified_df.pivot(index=\"hour_only\",columns=\"day\",values=\"Location\")\n",
    "\n",
    "        final_list=[]\n",
    "        #         print(modified_df_pivot.head(5))\n",
    "        modified_df_pivot=modified_df_pivot[cats]\n",
    "        #     print(modified_df_pivot)\n",
    "        for index, row in modified_df_pivot.iterrows():\n",
    "            temp_list=row.tolist()\n",
    "        #         temp_list=[round(i,3) for i in temp_list]\n",
    "            final_list.append(temp_list)\n",
    "\n",
    "\n",
    "        return final_list\n",
    "    \n",
    "def matrix_creation_total_duration(user,day_week_hour_minutes):\n",
    "    cats=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "    matrix_day_hour_minutes=[[None,None,None,None,None,None,None] for i in repeat(None, 24)]\n",
    "    user_data=\"\"\n",
    "    if user in day_week_hour_minutes:\n",
    "        user_data=day_week_hour_minutes[user]\n",
    "    if len(user_data)>0:\n",
    "        for day in cats:\n",
    "            if day in user_data:\n",
    "                for hour in range(24):\n",
    "                    matrix_day_hour_minutes[hour][cats.index(day)]=user_data[day][hour]\n",
    "            else:\n",
    "                for hour in range(24):\n",
    "                    matrix_day_hour_minutes[hour][cats.index(day)]=0\n",
    "        return matrix_day_hour_minutes\n",
    "    else:\n",
    "        return matrix_day_hour_minutes\n",
    "\n",
    "user_matrix_duration={}  \n",
    "user_matrix_stressor={}\n",
    "user_matrix_likelihood={}\n",
    "user_matrix_location={}\n",
    "user_matrix_day_hour_minutes={}\n",
    "for user in users_today:\n",
    "    \n",
    "    user_matrix_duration[user]=matrix_creation_duration(user,final_df_stressed)\n",
    "    user_matrix_stressor[user]=matrix_creation_stressor(user,final_df_stressed)\n",
    "    user_matrix_likelihood[user]=matrix_creation_likelihood(user,final_df_stressed)\n",
    "    user_matrix_location[user]=matrix_creation_location(user,final_df_stressed)\n",
    "    user_matrix_day_hour_minutes[user]=matrix_creation_total_duration(user,day_week_hour_minutes)\n",
    "    \n",
    "for user in user_matrix_stressor:\n",
    "    if user_matrix_stressor[user] !=None:\n",
    "        for i in user_matrix_stressor[user]:\n",
    "            for n,j in enumerate(i):\n",
    "                if j == 0:\n",
    "                    i[n]=None\n",
    "                \n",
    "# for user in user_matrix_duration:\n",
    "#     if user_matrix_duration[user] !=None:\n",
    "#         for i in user_matrix_duration[user]:\n",
    "#             for n,j in enumerate(i):\n",
    "#                 if j == 0:\n",
    "#                     i[n]=None\n",
    "                \n",
    "for user in user_matrix_likelihood:\n",
    "    if user_matrix_likelihood[user] !=None:\n",
    "        for i in user_matrix_likelihood[user]:\n",
    "            for n,j in enumerate(i):\n",
    "                if j == 0:\n",
    "                    i[n]=None\n",
    "                \n",
    "for user in user_matrix_location:\n",
    "    if user_matrix_location[user] !=None:\n",
    "        for i in user_matrix_location[user]:\n",
    "            for n,j in enumerate(i):\n",
    "                if j == 0:\n",
    "                    i[n]=None\n",
    "                    \n",
    "                    \n",
    "import pickle\n",
    "\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/user_matrix_duration.pickle', 'wb') as handle:\n",
    "    pickle.dump(user_matrix_duration, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# print(user_df)\n",
    "\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/user_matrix_stressor.pickle', 'wb') as handle:\n",
    "    pickle.dump(user_matrix_stressor, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/user_matrix_likelihood.pickle', 'wb') as handle:\n",
    "    pickle.dump(user_matrix_likelihood, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/user_matrix_location.pickle', 'wb') as handle:\n",
    "    pickle.dump(user_matrix_location, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/user_matrix_day_hour_minutes.pickle', 'wb') as handle:\n",
    "    pickle.dump(user_matrix_day_hour_minutes, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************\n",
      "data creating file finished\n",
      "****************************\n"
     ]
    }
   ],
   "source": [
    "print(\"****************************\")\n",
    "print(\"data creating file finished\")\n",
    "print(\"****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(users_today))\n",
    "with open('/home/jupyter/sneupane/MOODS/pickled_files/batch_users.pickle', 'rb') as handle:\n",
    "    batch_users = pickle.load(handle)\n",
    "    \n",
    "for batch in batch_users:\n",
    "    temp_users_batch=batch_users[batch]\n",
    "    for user in temp_users_batch:\n",
    "        if user in users_today:\n",
    "            print(user + \"--batch: \"+ str(batch) + \", week : \" + str(user_profile[user][5]), )\n",
    "#     users_today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_matrix_stressor[\"87d7d705-1cdf-36ae-9554-81afff640543\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df_stressed.loc[final_df_stressed[\"user\"]==\"ec9f72a3-fd99-3871-9434-bce64155d8bc\"][\"Stressor\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stress_id</th>\n",
       "      <th>starttime</th>\n",
       "      <th>endtime</th>\n",
       "      <th>value</th>\n",
       "      <th>confidence</th>\n",
       "      <th>episode_class</th>\n",
       "      <th>deleted</th>\n",
       "      <th>state</th>\n",
       "      <th>user_generated</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>...</th>\n",
       "      <th>start_time_of_day</th>\n",
       "      <th>end_time_of_day</th>\n",
       "      <th>start_day</th>\n",
       "      <th>end_day</th>\n",
       "      <th>start_hour</th>\n",
       "      <th>end_hour</th>\n",
       "      <th>days_diff</th>\n",
       "      <th>duration</th>\n",
       "      <th>counter</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>210592</th>\n",
       "      <td>46683</td>\n",
       "      <td>2022-02-13 20:27:28</td>\n",
       "      <td>2022-02-13 20:33:27</td>\n",
       "      <td>0.366296</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Not stressed</td>\n",
       "      <td>...</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Evening</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>5.983333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210594</th>\n",
       "      <td>46682</td>\n",
       "      <td>2022-02-13 20:23:26</td>\n",
       "      <td>2022-02-13 20:27:28</td>\n",
       "      <td>0.366024</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Not stressed</td>\n",
       "      <td>...</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Evening</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>4.033333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210595</th>\n",
       "      <td>46681</td>\n",
       "      <td>2022-02-13 20:16:27</td>\n",
       "      <td>2022-02-13 20:23:26</td>\n",
       "      <td>0.321200</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Evening</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>6.983333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210597</th>\n",
       "      <td>46680</td>\n",
       "      <td>2022-02-13 20:00:23</td>\n",
       "      <td>2022-02-13 20:16:27</td>\n",
       "      <td>0.534219</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Evening</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>16.066667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210601</th>\n",
       "      <td>46679</td>\n",
       "      <td>2022-02-13 19:47:32</td>\n",
       "      <td>2022-02-13 19:56:23</td>\n",
       "      <td>0.722288</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Evening</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>8.850000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28800</th>\n",
       "      <td>228392</td>\n",
       "      <td>2023-02-06 09:31:47</td>\n",
       "      <td>2023-02-06 10:02:52</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Morning</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>31.083333</td>\n",
       "      <td>359</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28804</th>\n",
       "      <td>228391</td>\n",
       "      <td>2023-02-06 09:18:47</td>\n",
       "      <td>2023-02-06 09:31:47</td>\n",
       "      <td>0.010019</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Morning</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>359</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28816</th>\n",
       "      <td>228390</td>\n",
       "      <td>2023-02-06 08:00:46</td>\n",
       "      <td>2023-02-06 09:18:47</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Morning</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>78.016667</td>\n",
       "      <td>359</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28818</th>\n",
       "      <td>228389</td>\n",
       "      <td>2023-02-06 07:37:47</td>\n",
       "      <td>2023-02-06 07:49:47</td>\n",
       "      <td>0.091069</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Morning</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>359</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28822</th>\n",
       "      <td>228388</td>\n",
       "      <td>2023-02-06 07:19:47</td>\n",
       "      <td>2023-02-06 07:37:47</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Morning</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>359</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6396 rows  36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stress_id           starttime             endtime     value  \\\n",
       "210592      46683 2022-02-13 20:27:28 2022-02-13 20:33:27  0.366296   \n",
       "210594      46682 2022-02-13 20:23:26 2022-02-13 20:27:28  0.366024   \n",
       "210595      46681 2022-02-13 20:16:27 2022-02-13 20:23:26  0.321200   \n",
       "210597      46680 2022-02-13 20:00:23 2022-02-13 20:16:27  0.534219   \n",
       "210601      46679 2022-02-13 19:47:32 2022-02-13 19:56:23  0.722288   \n",
       "...           ...                 ...                 ...       ...   \n",
       "28800      228392 2023-02-06 09:31:47 2023-02-06 10:02:52  0.000857   \n",
       "28804      228391 2023-02-06 09:18:47 2023-02-06 09:31:47  0.010019   \n",
       "28816      228390 2023-02-06 08:00:46 2023-02-06 09:18:47  0.000814   \n",
       "28818      228389 2023-02-06 07:37:47 2023-02-06 07:49:47  0.091069   \n",
       "28822      228388 2023-02-06 07:19:47 2023-02-06 07:37:47  0.000114   \n",
       "\n",
       "        confidence  episode_class  deleted state  user_generated  \\\n",
       "210592         0.5            2.0        0                     0   \n",
       "210594         0.5            2.0        0                     0   \n",
       "210595         0.5            0.0        0                     0   \n",
       "210597         0.5            3.0        0                     0   \n",
       "210601         0.5            3.0        0                     0   \n",
       "...            ...            ...      ...   ...             ...   \n",
       "28800          0.5            0.0        0                     0   \n",
       "28804          0.5            0.0        0                     0   \n",
       "28816          0.5            0.0        0                     0   \n",
       "28818          0.5            0.0        0                     0   \n",
       "28822          0.5            0.0        0                     0   \n",
       "\n",
       "         user_rating  ... start_time_of_day  end_time_of_day  start_day  \\\n",
       "210592  Not stressed  ...           Evening          Evening 2022-02-13   \n",
       "210594  Not stressed  ...           Evening          Evening 2022-02-13   \n",
       "210595          None  ...           Evening          Evening 2022-02-13   \n",
       "210597          None  ...           Evening          Evening 2022-02-13   \n",
       "210601          None  ...           Evening          Evening 2022-02-13   \n",
       "...              ...  ...               ...              ...        ...   \n",
       "28800           None  ...           Morning          Morning 2023-02-06   \n",
       "28804           None  ...           Morning          Morning 2023-02-06   \n",
       "28816           None  ...           Morning          Morning 2023-02-06   \n",
       "28818           None  ...           Morning          Morning 2023-02-06   \n",
       "28822           None  ...           Morning          Morning 2023-02-06   \n",
       "\n",
       "          end_day  start_hour  end_hour  days_diff   duration counter week  \n",
       "210592 2022-02-13          20        20          0   5.983333       1    1  \n",
       "210594 2022-02-13          20        20          0   4.033333       1    1  \n",
       "210595 2022-02-13          20        20          0   6.983333       1    1  \n",
       "210597 2022-02-13          20        20          0  16.066667       1    1  \n",
       "210601 2022-02-13          19        19          0   8.850000       1    1  \n",
       "...           ...         ...       ...        ...        ...     ...  ...  \n",
       "28800  2023-02-06           9        10          0  31.083333     359   51  \n",
       "28804  2023-02-06           9         9          0  13.000000     359   51  \n",
       "28816  2023-02-06           8         9          0  78.016667     359   51  \n",
       "28818  2023-02-06           7         7          0  12.000000     359   51  \n",
       "28822  2023-02-06           7         7          0  18.000000     359   51  \n",
       "\n",
       "[6396 rows x 36 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.loc[final_df[\"user\"]==\"c123b0f3-a897-38af-b44e-d16cee589f7f\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3.3",
   "language": "python",
   "name": "cc33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
